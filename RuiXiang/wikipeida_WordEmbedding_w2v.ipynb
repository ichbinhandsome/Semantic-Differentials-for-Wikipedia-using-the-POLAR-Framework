{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "# Word Embedding from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "# import string\n",
    "\n",
    "def preprocessing(article):\n",
    "    # define stopwords, punctuations, lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    # find all english words from article\n",
    "    words = re.findall('\\\\b[a-z]+\\\\b',article)\n",
    "    clean_words = []\n",
    "    # delete all stopwords and creat a clean words_list\n",
    "    for word in words:\n",
    "        if word not in stopwords_set:\n",
    "            clean_words.append(word)\n",
    "    #Lemmatiztion\n",
    "    filtered_words = [lemmatizer.lemmatize(word,'n') for word in clean_words] \n",
    "    filtered_article = ' '.join(filtered_words)\n",
    "    return filtered_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85102it [06:17, 225.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# clean original wiki_corpus and put cleaned content into a new file\n",
    "\n",
    "# each line in this text file corresponding to an article\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"./Dataset/wiki_corpus_en.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for line in tqdm(f):\n",
    "        article = line.strip('\\n')\n",
    "        filtered_article = preprocessing(article)\n",
    "        with open('./Dataset/wiki_processed_corpus_en.txt', 'a', encoding='utf-8') as w:\n",
    "            w.write(filtered_article + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gensim model to train word2vec embeddings\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Word2Vec training.....\n"
     ]
    }
   ],
   "source": [
    "sentences = LineSentence('./Dataset/wiki_processed_corpus_en.txt')\n",
    "\n",
    "#define and train the gensim model \n",
    "model = Word2Vec(sentences, sg=1, # using skip-gram model\n",
    "                     size= 300,  # vector size 300\n",
    "                     window= 5, # context window size\n",
    "                     min_count= 5, # minimal word frequency\n",
    "                     iter = 15, # itertaion 15 times\n",
    "                     workers= 8 )\n",
    "\n",
    "print('Finished Word2Vec training.....')\n",
    "\n",
    "model.save('./Dataset/word2vec.model')\n",
    "model.wv.save_word2vec_format(\"./Dataset/word2vec.txt\",binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01883121  0.20953107 -0.06348649  0.12446711 -0.30815777 -0.06482673\n",
      " -0.24392161 -0.3617024   0.03132427  0.12887119  0.28699267 -0.30248684\n",
      "  0.14022109 -0.01792672  0.06673583  0.08856788 -0.01744785  0.03064011\n",
      "  0.19901419 -0.21329336 -0.13122673 -0.1792982  -0.1827066  -0.17658928\n",
      " -0.07008389  0.18022831  0.31833333 -0.1922571   0.25560144 -0.15702173\n",
      "  0.1104394   0.07718533 -0.30509222  0.11992095  0.18463781 -0.06511118\n",
      " -0.0698548   0.38524476 -0.05967232 -0.19643001 -0.05561528  0.03574706\n",
      "  0.01241283  0.12788253  0.30667087  0.3243983   0.10205039 -0.42644563\n",
      "  0.21918465  0.13985078  0.02472307 -0.03273129  0.01207019 -0.09052381\n",
      "  0.21873304  0.03180779  0.20899987 -0.15861568 -0.14080344 -0.07981589\n",
      " -0.06710944  0.20989531  0.05883698 -0.09836747  0.07262119 -0.09066577\n",
      "  0.15727943  0.13546565 -0.27404085  0.23382223 -0.06612139 -0.02006742\n",
      "  0.10213558 -0.07222913 -0.33423388  0.03667687 -0.22206816  0.37079248\n",
      "  0.09546407 -0.00987136 -0.13475263 -0.03398062  0.08661698  0.4833384\n",
      "  0.23269334 -0.02351934  0.04562251  0.14247778  0.16650325 -0.00765055\n",
      " -0.10899193  0.21743281  0.2687554  -0.22337161 -0.1187861   0.07109492\n",
      " -0.19593245 -0.0010209   0.09031998 -0.04796755 -0.14908062  0.06977087\n",
      " -0.17531118  0.04788121 -0.01375512 -0.04337248 -0.15877679  0.04825294\n",
      " -0.1615656   0.37295622  0.33325997 -0.35528398  0.28259137  0.03028546\n",
      " -0.3492493  -0.0362286   0.04713215 -0.2974725   0.12777252 -0.12562056\n",
      "  0.04390784  0.10037654  0.18456571  0.31651017 -0.28330117 -0.33608106\n",
      " -0.2551284   0.00171842  0.0244219  -0.19971786  0.09015547  0.532765\n",
      " -0.042441   -0.02003682 -0.1458123   0.20969318  0.07433219  0.3758945\n",
      " -0.10000116  0.32741603 -0.36118567  0.0285657   0.3164782  -0.15492436\n",
      " -0.31339994  0.15274513  0.083814   -0.13444954  0.15205914  0.16731122\n",
      "  0.01468655  0.2030132   0.1729166  -0.12820241  0.06814604 -0.05790499\n",
      " -0.32612172  0.13892892  0.0618008   0.27012557 -0.11827549 -0.01706284\n",
      "  0.16557889  0.21709149 -0.42584535  0.11202031  0.3788501  -0.09066511\n",
      "  0.1489031   0.0990989   0.01590822  0.32209927 -0.5106212  -0.26627636\n",
      " -0.25918883 -0.10748848  0.1232212  -0.0315056   0.22129966  0.2774253\n",
      " -0.3004357   0.16748875 -0.04278236 -0.04457603 -0.19065838  0.12092789\n",
      "  0.12694298  0.3405706  -0.19881125 -0.1570506   0.07893201  0.01633201\n",
      " -0.12019806  0.2769567  -0.17181939  0.23945037  0.14483291  0.24628744\n",
      " -0.22581363 -0.00502691  0.01176238 -0.11246756 -0.13327596  0.16935296\n",
      " -0.11681525  0.04794695  0.19516177  0.17074542 -0.06523983 -0.1843843\n",
      " -0.01366916 -0.11434906  0.21572833  0.20389    -0.10004354 -0.41598856\n",
      "  0.06181294  0.06822757 -0.25980562  0.58520496 -0.12383796 -0.17841668\n",
      " -0.06697243  0.0211216   0.17743076  0.05186253 -0.24674174 -0.04653821\n",
      "  0.00216237  0.09028018  0.16348886  0.12076     0.3069685  -0.0986002\n",
      " -0.18168102  0.22051515 -0.36075443 -0.52993524 -0.14711288 -0.46015862\n",
      " -0.17465565  0.06958733  0.4875378   0.48628002  0.36039466 -0.15500954\n",
      " -0.24891973  0.23384932 -0.13743319 -0.00532784  0.22823668  0.12794948\n",
      "  0.17832673  0.02689315 -0.17597197  0.09863663 -0.06963111 -0.26967934\n",
      "  0.33987275  0.04210239  0.12581645  0.16454822 -0.13355048 -0.05853893\n",
      "  0.17038582 -0.04242357  0.13227092 -0.27081266 -0.07857037  0.5425683\n",
      "  0.22053474  0.04123517 -0.16101712 -0.31764454  0.1520922  -0.129803\n",
      "  0.20386495  0.26333448  0.38114515  0.43988353  0.27632692 -0.04298362\n",
      "  0.20449054 -0.42167544 -0.12235066  0.30678353 -0.0346421   0.05689107\n",
      " -0.2536877  -0.2888778   0.20339197 -0.06343731  0.13927376 -0.07721771\n",
      "  0.09088933 -0.05581054  0.09090766  0.21172802  0.43867368 -0.20497364]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangr\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('taiwan', 0.6887751221656799),\n",
       " ('chinese', 0.6543804407119751),\n",
       " ('zhangzong', 0.6423479914665222),\n",
       " ('putian', 0.6366764307022095),\n",
       " ('qing', 0.6274092197418213),\n",
       " ('guangxi', 0.6232026815414429),\n",
       " ('yongji', 0.6203204393386841),\n",
       " ('beijing', 0.6113041639328003),\n",
       " ('korea', 0.6085971593856812),\n",
       " ('aizong', 0.6058491468429565)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test word vector\n",
    "wv_from_text = KeyedVectors.load_word2vec_format(\"./Dataset/word2vec.txt\", binary=False)\n",
    "print(wv_from_text['germany'])\n",
    "model_trained = Word2Vec.load('./Dataset/word2vec.model')\n",
    "model_trained.most_similar(\"china\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangr\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('iraq', 0.6985960006713867),\n",
       " ('afghan', 0.6625270247459412),\n",
       " ('kabul', 0.647105872631073),\n",
       " ('taliban', 0.6283484697341919),\n",
       " ('wardak', 0.6166841387748718),\n",
       " ('helmand', 0.6157212257385254),\n",
       " ('iran', 0.602006733417511),\n",
       " ('gardez', 0.5908187627792358),\n",
       " ('iraqi', 0.5878793001174927),\n",
       " ('paktika', 0.584496021270752)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trained.most_similar(\"afghanistan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missed countries: \n",
      "guinea-bissau  is not in our trained vocabulary model\n",
      "laos  is not in our trained vocabulary model\n",
      "n. mariana islands  is not in our trained vocabulary model\n",
      "philippines  is not in our trained vocabulary model\n",
      "turks & caicos is  is not in our trained vocabulary model\n"
     ]
    }
   ],
   "source": [
    "# find countries vector and store it\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./Dataset/countries/countries of the world.csv')\n",
    "\n",
    "country2vector = {}\n",
    "\n",
    "print('missed countries: ')\n",
    "for country in df['Country']:\n",
    "    c = str(country.strip()).lower()\n",
    "    country = c.split(' ')[0]\n",
    "    country = country.split(',')[0]\n",
    "    try:\n",
    "        country2vector[country] = wv_from_text[country]\n",
    "    except:\n",
    "        print(c, ' is not in our trained vocabulary model')\n",
    "        \n",
    "df_cou2vec = pd.DataFrame({'country': list(country2vector.keys()),'vector': list(country2vector.values()) })\n",
    "df_cou2vec.to_csv('./Dataset/countries/country2vector.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    afghanistan\n",
      "1        albania\n",
      "2        algeria\n",
      "3       american\n",
      "4        andorra\n",
      "Name: country, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_c = pd.read_csv('./Dataset/countries/country2vector.csv')\n",
    "print(df_c.head()['country'])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
