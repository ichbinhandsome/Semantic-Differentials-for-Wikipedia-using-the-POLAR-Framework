{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate new antonym pairs for specific topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "#loading trained model\n",
    "model_norm = KeyedVectors.load_word2vec_format(\"./Dataset/word2vec_norm\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading original antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_antonym = []\n",
    "\n",
    "with open('./Dataset/Antonym_sets/LenciBenotto.val') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "\n",
    "\n",
    "with open('./Dataset/Antonym_sets/LenciBenotto.test') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "with open('./Dataset/Antonym_sets/EVALution.val') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "with open('./Dataset/Antonym_sets/EVALution.test') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "                \n",
    "list_antonym = list(dict.fromkeys(list_antonym).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = defaultdict(list)\n",
    "for each_pair in tqdm(list_antonym):\n",
    "    word1 = each_pair[0]\n",
    "    word2 = each_pair[1]\n",
    "    if word1 < word2:\n",
    "        similarity_matrix[word1].append(word2)\n",
    "    else:\n",
    "        similarity_matrix[word2].append(word1)\n",
    "    \n",
    "all_similarity = defaultdict(dict)\n",
    "for each_key in tqdm(similarity_matrix):\n",
    "    for each_value in similarity_matrix[each_key]:\n",
    "#         cosine_similarity([current_model[each_key]]\n",
    "        all_similarity[each_key][each_value] = abs(cosine_similarity([current_model[each_key]],[current_model[each_value]])[0][0])\n",
    "    \n",
    "final_antonym_list = []\n",
    "for index_counter, each_key in enumerate(tqdm(all_similarity)):\n",
    "#     print(each_key,all_similarity[each_key])\n",
    "    listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "#     print(listofTuples)\n",
    "    final_antonym_list.append((each_key, listofTuples[0][0]))\n",
    "print(len(final_antonym_list))\n",
    "\n",
    "list_antonym = final_antonym_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store original antonyms\n",
    "with open('./all_antonyms', 'w') as f:\n",
    "    f.writelines([i+' ' +j+'\\n' for i,j in list_antonym])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use original antonyms to find the most similar word pairs related to specific topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "antonyms = {}\n",
    "with open('./all_antonyms', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        word_pair = line.split()\n",
    "        antonyms[i] = word_pair\n",
    "\n",
    "def select_antonym(topic, antonyms, n, model):\n",
    "    similarity_score = defaultdict(float)\n",
    "    for i, words in antonyms.items():\n",
    "        # similarity_score[i] = max(model.similarity(topic,words[0]),model.similarity(topic,words[0]))\n",
    "        similarity_score[i] = model.similarity(topic,words[0]) + model.similarity(topic,words[0])\n",
    "    selected_antonyms = []\n",
    "    sorted_similarity_score = sorted(similarity_score.items(), key = lambda x:x[1], reverse=True)\n",
    "    for item in sorted_similarity_score[:n]:\n",
    "        selected_antonyms.append(antonyms[item[0]])\n",
    "    return selected_antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate antonyms for topic 'country'\n",
    "select_country = select_antonym('country', antonyms, 300, model_norm)\n",
    "select_china = select_antonym('china', antonyms, 300, model_norm)\n",
    "select_usa = select_antonym('germany', antonyms, 200, model_norm)\n",
    "\n",
    "re = []\n",
    "for i in select_country:\n",
    "    if i in select_china:\n",
    "        re.append(i)\n",
    "\n",
    "#store selectd antonyms\n",
    "# with open('./generated_words/CountryWordPairsByHand','w') as f:\n",
    "#     f.writelines(i+' '+j +'\\n' for i,j in re)\n",
    "\n",
    "# for i in select_country:\n",
    "#     if i in select_usa and i not in re:\n",
    "#         print(i[0],i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use selected antonyms as root to generate new antonyms by Wordnet and API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load root antonyms\n",
    "with open('./generated_antonyms/CountryWordPairsByHand','r') as f:\n",
    "    root = [line.strip('\\n').split(' ') for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case cell\n",
    "#########################\n",
    "# from nltk.corpus import wordnet as wn\n",
    "# wordsets = wn.synsets('good')\n",
    "# for word in wordsets:\n",
    "#     if word.hyponyms():\n",
    "#         hypon = word.hyponyms()\n",
    "#         for h in hypon:\n",
    "#             for w in h.lemmas():\n",
    "#                 if w.antonyms():\n",
    "#                     print(w.name(), w.antonyms()[0].name())\n",
    "# re = [sets.name().split('.')[0] for sets in wordsets]\n",
    "# print(sets.lemmas()[0].antonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate antonyms from wordnet\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def generate_antonyms_wordnet(root):\n",
    "    res = []\n",
    "    for antonyms in root:\n",
    "        #generate antonym pair from synonyms\n",
    "        wordsets = wn.synsets(antonyms[0])\n",
    "        for sets in wordsets:\n",
    "            for w in sets.lemmas():\n",
    "                word1 = w.name()\n",
    "                if w.antonyms():\n",
    "                    word2 = w.antonyms()[0].name()\n",
    "                    word_pair = [word1, word2]\n",
    "                    if word_pair not in res:\n",
    "                        res.append(word_pair)\n",
    "        wordsets = wn.synsets(antonyms[1])\n",
    "        for sets in wordsets:\n",
    "            for w in sets.lemmas():\n",
    "                word1 = w.name()\n",
    "                if w.antonyms():\n",
    "                    word2 = w.antonyms()[0].name()\n",
    "                    word_pair = [word1, word2]\n",
    "                    if word_pair not in res:\n",
    "                        res.append(word_pair)\n",
    "        #generate antonym pair from hyponyms\n",
    "        wordsets = wn.synsets(antonyms[0])\n",
    "        for sets in wordsets:\n",
    "            if sets.hyponyms():\n",
    "                hypon = sets.hyponyms()\n",
    "                for h in hypon:\n",
    "                    for w in h.lemmas():\n",
    "                        word1 = w.name()\n",
    "                        if w.antonyms():\n",
    "                            word2 = w.antonyms()[0].name()\n",
    "                            word_pair = [word1, word2]\n",
    "                            if word_pair not in res:\n",
    "                                 res.append(word_pair)\n",
    "        wordsets = wn.synsets(antonyms[1])\n",
    "        for sets in wordsets:\n",
    "            if sets.hyponyms():\n",
    "                hypon = sets.hyponyms()\n",
    "                for h in hypon:\n",
    "                    for w in h.lemmas():\n",
    "                        word1 = w.name()\n",
    "                        if w.antonyms():\n",
    "                            word2 = w.antonyms()[0].name()\n",
    "                            word_pair = [word1, word2]\n",
    "                            if word_pair not in res:\n",
    "                                res.append(word_pair)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "antonyms_wordnet = generate_antonyms_wordnet(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate antonyms from API: http://www.thesaurus.com\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def find_synonyms(term):\n",
    "    response = requests.get('http://www.thesaurus.com/browse/{}'.format(term))\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    section = soup.find('ul', {'class': 'css-17d6qyx-WordGridLayoutBox et6tpn80'})\n",
    "    if section:\n",
    "        # find 10 synonyms\n",
    "        return [li.text for li in section.findAll('li')][:10]\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def find_antonyms(term):\n",
    "    response = requests.get('http://www.thesaurus.com/browse/{}'.format(term))\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    soup('ul', {'class': 'css-17d6qyx-WordGridLayoutBox et6tpn80'})[0].extract()\n",
    "    section = soup.find('ul', {'class': 'css-17d6qyx-WordGridLayoutBox et6tpn80'})\n",
    "    if section:\n",
    "        # find 3 antonyms\n",
    "        return [li.text for li in section.findAll('li')][:3]\n",
    "    else:\n",
    "        return \n",
    "\n",
    "def generate_antonyms_api(root):\n",
    "    res = []\n",
    "    for antonyms in root:\n",
    "        sy_1 = find_synonyms(antonyms[0])\n",
    "        sy_2 = find_synonyms(antonyms[1])\n",
    "        for s in sy_1:\n",
    "            ant = find_antonyms(s)\n",
    "            if ant:\n",
    "                for a in ant:\n",
    "                    word_pair = [s, a]\n",
    "                    res.append(word_pair)\n",
    "        for s in sy_2:\n",
    "            ant = find_antonyms(s)\n",
    "            if ant:\n",
    "                for a in ant:\n",
    "                    word_pair = [s, a]\n",
    "                    res.append(word_pair)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 7min 55s\n"
    }
   ],
   "source": [
    "%%time\n",
    "antonyms_api = generate_antonyms_api(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with generated antonym pairs\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# first method: naive (for generated antonyms from wordnet)\n",
    "def process_antonyms(antonyms, trained_model):\n",
    "    # just reserve the unique word pairs which are contained in trained_model\n",
    "    hash_set = set()\n",
    "    for wordpair in antonyms:\n",
    "        word1, word2 = wordpair\n",
    "        if word1 in trained_model and word2 in trained_model:\n",
    "            if word1 > word2:\n",
    "                word1, word2 = word2, word1\n",
    "            hash_set.add((word1, word2))\n",
    "    print(len(hash_set))    \n",
    "    return list(hash_set)\n",
    "\n",
    "#second method: more powerful (for generated antonyms from api)\n",
    "def process_antonyms_max(antonyms, current_model):\n",
    "    hash_set = set()\n",
    "    for wordpair in antonyms:\n",
    "        word1, word2 = wordpair\n",
    "        if word1 in current_model and word2 in current_model:\n",
    "            if word1 > word2:\n",
    "                word1, word2 = word2, word1\n",
    "            hash_set.add((word1, word2))\n",
    "    list_antonym = list(hash_set)\n",
    "\n",
    "    similarity_matrix = defaultdict(list)\n",
    "    for each_pair in tqdm(list_antonym):\n",
    "        word1 = each_pair[0]\n",
    "        word2 = each_pair[1]\n",
    "        if word1 < word2:\n",
    "            similarity_matrix[word1].append(word2)\n",
    "        else:\n",
    "            similarity_matrix[word2].append(word1)\n",
    "\n",
    "    all_similarity = defaultdict(dict)\n",
    "    for each_key in tqdm(similarity_matrix):\n",
    "        for each_value in similarity_matrix[each_key]:\n",
    "            all_similarity[each_key][each_value] = abs(cosine_similarity([current_model[each_key]],[current_model[each_value]])[0][0])\n",
    "\n",
    "    final_antonym_list = []\n",
    "    for index_counter, each_key in enumerate(tqdm(all_similarity)):\n",
    "        listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "        final_antonym_list.append((each_key, listofTuples[0][0]))\n",
    "\n",
    "    print(len(final_antonym_list))\n",
    "    return final_antonym_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "146\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=2320), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1447fee4c974dd2a7ea4ebd507f00f2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=1023), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbc8fe8a8dc94fd5a2e8688daa319955"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=1023), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56b3be47dc614b8a9ff83ef427aa6bc8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n1023\n"
    }
   ],
   "source": [
    "processed_antonyms_wordnet = process_antonyms(antonyms_wordnet, model_norm)\n",
    "processed_antonyms_api = process_antonyms_max(antonyms_api, model_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_antonyms(root, processed_antonyms_wordnet, processed_antonyms_api, current_model):\n",
    "    hash_set = set()\n",
    "    for wordpair in root:\n",
    "        word1, word2 = wordpair\n",
    "        hash_set.add((word1, word2))\n",
    "    for wordpair in processed_antonyms_wordnet:\n",
    "        word1, word2 = wordpair\n",
    "        hash_set.add((word1, word2))\n",
    "    for wordpair in processed_antonyms_api:\n",
    "        word1, word2 = wordpair\n",
    "        hash_set.add((word1, word2))\n",
    "\n",
    "    list_antonym = list(hash_set)\n",
    "\n",
    "    similarity_matrix = defaultdict(list)\n",
    "    for each_pair in list_antonym:\n",
    "        word1 = each_pair[0]\n",
    "        word2 = each_pair[1]\n",
    "        if word1 < word2:\n",
    "            similarity_matrix[word1].append(word2)\n",
    "        else:\n",
    "            similarity_matrix[word2].append(word1)\n",
    "\n",
    "    all_similarity = defaultdict(dict)\n",
    "    for each_key in similarity_matrix:\n",
    "        for each_value in similarity_matrix[each_key]:\n",
    "            all_similarity[each_key][each_value] = abs(cosine_similarity([current_model[each_key]],[current_model[each_value]])[0][0])\n",
    "\n",
    "    final_antonym_list = []\n",
    "    for index_counter, each_key in enumerate(all_similarity):\n",
    "        listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "        final_antonym_list.append((each_key, listofTuples[0][0]))\n",
    "\n",
    "    print(len(final_antonym_list))\n",
    "    return final_antonym_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1141\n"
    }
   ],
   "source": [
    "final_antonym_list = collect_antonyms(root, processed_antonyms_wordnet, processed_antonyms_api, model_norm)\n",
    "#store final antonyms\n",
    "with open('./generated_antonyms/country_related_antonyms', 'w') as f:\n",
    "    f.writelines([i+' ' +j+'\\n' for i,j in final_antonym_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('close', 'expand'),\n ('help', 'partition'),\n ('fighting', 'nice'),\n ('executive', 'subordinate'),\n ('consumer', 'seller'),\n ('bountiful', 'insufficient'),\n ('commitment', 'detachment'),\n ('awful', 'beautiful'),\n ('current', 'old'),\n ('ambiguous', 'direct'),\n ('actuality', 'forfeit'),\n ('chore', 'peace'),\n ('erudition', 'ignorance'),\n ('good', 'nefarious'),\n ('structured', 'unstructured'),\n ('uncultured', 'urbane'),\n ('gloss', 'misinformation'),\n ('endangerment', 'safeguard'),\n ('opponent', 'patron'),\n ('huge', 'microscopic'),\n ('expert', 'neophyte'),\n ('distribute', 'gather'),\n ('lose', 'profit'),\n ('avoidance', 'encounter'),\n ('agitation', 'silence'),\n ('encumbrance', 'help'),\n ('extreme', 'space'),\n ('float', 'sink'),\n ('indefinite', 'secure'),\n ('antagonistic', 'kind')]"
     },
     "metadata": {},
     "execution_count": 171
    }
   ],
   "source": [
    "final_antonym_list[:30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}