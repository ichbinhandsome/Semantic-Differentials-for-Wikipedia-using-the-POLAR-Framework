{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate new antonym pairs for specific topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "#loading trained model\n",
    "model_norm = KeyedVectors.load_word2vec_format(\"./Dataset/word2vec_norm\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading original antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_antonym = []\n",
    "\n",
    "with open('./Dataset/Antonym_sets/LenciBenotto.val') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "\n",
    "\n",
    "with open('./Dataset/Antonym_sets/LenciBenotto.test') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "with open('./Dataset/Antonym_sets/EVALution.val') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "with open('./Dataset/Antonym_sets/EVALution.test') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "                \n",
    "list_antonym = list(dict.fromkeys(list_antonym).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = defaultdict(list)\n",
    "for each_pair in tqdm(list_antonym):\n",
    "    word1 = each_pair[0]\n",
    "    word2 = each_pair[1]\n",
    "    if word1 < word2:\n",
    "        similarity_matrix[word1].append(word2)\n",
    "    else:\n",
    "        similarity_matrix[word2].append(word1)\n",
    "    \n",
    "all_similarity = defaultdict(dict)\n",
    "for each_key in tqdm(similarity_matrix):\n",
    "    for each_value in similarity_matrix[each_key]:\n",
    "#         cosine_similarity([current_model[each_key]]\n",
    "        all_similarity[each_key][each_value] = abs(cosine_similarity([current_model[each_key]],[current_model[each_value]])[0][0])\n",
    "    \n",
    "final_antonym_list = []\n",
    "for index_counter, each_key in enumerate(tqdm(all_similarity)):\n",
    "#     print(each_key,all_similarity[each_key])\n",
    "    listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "#     print(listofTuples)\n",
    "    final_antonym_list.append((each_key, listofTuples[0][0]))\n",
    "print(len(final_antonym_list))\n",
    "\n",
    "list_antonym = final_antonym_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store original antonyms\n",
    "with open('./all_antonyms', 'w') as f:\n",
    "    f.writelines([i+' ' +j+'\\n' for i,j in list_antonym])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use original antonyms to find the most similar word pairs related to specific topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "antonyms = {}\n",
    "with open('./all_antonyms', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        word_pair = line.split()\n",
    "        antonyms[i] = word_pair\n",
    "\n",
    "def select_antonym(topic, antonyms, n, model):\n",
    "    similarity_score = defaultdict(float)\n",
    "    for i, words in antonyms.items():\n",
    "        # similarity_score[i] = max(model.similarity(topic,words[0]),model.similarity(topic,words[0]))\n",
    "        similarity_score[i] = model.similarity(topic,words[0]) + model.similarity(topic,words[0])\n",
    "    selected_antonyms = []\n",
    "    sorted_similarity_score = sorted(similarity_score.items(), key = lambda x:x[1], reverse=True)\n",
    "    for item in sorted_similarity_score[:n]:\n",
    "        selected_antonyms.append(antonyms[item[0]])\n",
    "    return selected_antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate antonyms for topic 'country'\n",
    "select_country = select_antonym('country', antonyms, 300, model_norm)\n",
    "select_china = select_antonym('china', antonyms, 300, model_norm)\n",
    "select_usa = select_antonym('germany', antonyms, 200, model_norm)\n",
    "\n",
    "re = []\n",
    "for i in select_country:\n",
    "    if i in select_china:\n",
    "        re.append(i)\n",
    "\n",
    "#store selectd antonyms\n",
    "# with open('./generated_words/CountryWordPairsByHand','w') as f:\n",
    "#     f.writelines(i+' '+j +'\\n' for i,j in re)\n",
    "\n",
    "# for i in select_country:\n",
    "#     if i in select_usa and i not in re:\n",
    "#         print(i[0],i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use selected antonyms as root to generate new antonyms by Wordnet and API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load root antonyms\n",
    "\n",
    "# with open('./generated_antonyms/CountryWordPairsByHand','r') as f:\n",
    "#     root = [line.strip('\\n').split(' ') for line in f.readlines()]\n",
    "\n",
    "with open('./generated_antonyms/country50_by_Jan.txt','r') as f:\n",
    "    root = [line.strip('\\n').split(' ') for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['east', 'west'],\n ['north', 'south'],\n ['europa', 'asia'],\n ['america', 'eurasien'],\n ['africa', 'eurasien'],\n ['australia', 'eurasien'],\n ['scandinavian', 'baltic'],\n ['large', 'small'],\n ['big', 'tiny'],\n ['long', 'wide'],\n ['black', 'white'],\n ['brown', 'white'],\n ['yellow', 'white'],\n ['grey', 'green'],\n ['grey', 'blue'],\n ['calm', 'busy'],\n ['cosmopolitan', 'limited'],\n ['touristy', 'limited'],\n ['america', 'russia'],\n ['america', 'china'],\n ['russia', 'china'],\n ['latino', 'hispanic'],\n ['english', 'russian'],\n ['english', 'chinese'],\n ['english', 'spanish'],\n ['spanish', 'portuguese'],\n ['developed', 'undeveloped'],\n ['first', 'third'],\n ['poor', 'rich'],\n ['civilized', 'uncivilized'],\n ['modern', 'traditional'],\n ['city', 'land'],\n ['farming', 'development'],\n ['catholic', 'muslim'],\n ['catholic', 'buddhism'],\n ['hindi', 'muslim'],\n ['democratic', 'monarchic'],\n ['hot', 'cold'],\n ['tropical', 'mediterane'],\n ['desert', 'vegetation']]"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case cell\n",
    "#########################\n",
    "# from nltk.corpus import wordnet as wn\n",
    "# wordsets = wn.synsets('good')\n",
    "# for word in wordsets:\n",
    "#     if word.hyponyms():\n",
    "#         hypon = word.hyponyms()\n",
    "#         for h in hypon:\n",
    "#             for w in h.lemmas():\n",
    "#                 if w.antonyms():\n",
    "#                     print(w.name(), w.antonyms()[0].name())\n",
    "# re = [sets.name().split('.')[0] for sets in wordsets]\n",
    "# print(sets.lemmas()[0].antonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate antonyms from wordnet\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def generate_antonyms_wordnet(root):\n",
    "    res = []\n",
    "    for antonyms in root:\n",
    "        #generate antonym pair from synonyms\n",
    "        wordsets = wn.synsets(antonyms[0])\n",
    "        for sets in wordsets:\n",
    "            for w in sets.lemmas():\n",
    "                word1 = w.name()\n",
    "                if w.antonyms():\n",
    "                    word2 = w.antonyms()[0].name()\n",
    "                    word_pair = [word1, word2]\n",
    "                    if word_pair not in res:\n",
    "                        res.append(word_pair)\n",
    "        wordsets = wn.synsets(antonyms[1])\n",
    "        for sets in wordsets:\n",
    "            for w in sets.lemmas():\n",
    "                word1 = w.name()\n",
    "                if w.antonyms():\n",
    "                    word2 = w.antonyms()[0].name()\n",
    "                    word_pair = [word1, word2]\n",
    "                    if word_pair not in res:\n",
    "                        res.append(word_pair)\n",
    "        #generate antonym pair from hyponyms\n",
    "        wordsets = wn.synsets(antonyms[0])\n",
    "        for sets in wordsets:\n",
    "            if sets.hyponyms():\n",
    "                hypon = sets.hyponyms()\n",
    "                for h in hypon:\n",
    "                    for w in h.lemmas():\n",
    "                        word1 = w.name()\n",
    "                        if w.antonyms():\n",
    "                            word2 = w.antonyms()[0].name()\n",
    "                            word_pair = [word1, word2]\n",
    "                            if word_pair not in res:\n",
    "                                 res.append(word_pair)\n",
    "        wordsets = wn.synsets(antonyms[1])\n",
    "        for sets in wordsets:\n",
    "            if sets.hyponyms():\n",
    "                hypon = sets.hyponyms()\n",
    "                for h in hypon:\n",
    "                    for w in h.lemmas():\n",
    "                        word1 = w.name()\n",
    "                        if w.antonyms():\n",
    "                            word2 = w.antonyms()[0].name()\n",
    "                            word_pair = [word1, word2]\n",
    "                            if word_pair not in res:\n",
    "                                res.append(word_pair)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "antonyms_wordnet = generate_antonyms_wordnet(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate antonyms from API: http://www.thesaurus.com\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def find_synonyms(term):\n",
    "    response = requests.get('http://www.thesaurus.com/browse/{}'.format(term))\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    section = soup.find('ul', {'class': 'css-17d6qyx-WordGridLayoutBox et6tpn80'})\n",
    "    if section:\n",
    "        # find 10 synonyms\n",
    "        return [li.text for li in section.findAll('li')][:10]\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def find_antonyms(term):\n",
    "    response = requests.get('http://www.thesaurus.com/browse/{}'.format(term))\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    if soup('ul', {'class': 'css-17d6qyx-WordGridLayoutBox et6tpn80'}):\n",
    "        soup('ul', {'class': 'css-17d6qyx-WordGridLayoutBox et6tpn80'})[0].extract()\n",
    "        section = soup.find('ul', {'class': 'css-17d6qyx-WordGridLayoutBox et6tpn80'})\n",
    "        if section:\n",
    "            # find 3 antonyms\n",
    "            return [li.text for li in section.findAll('li')][:3]\n",
    "        else:\n",
    "            return \n",
    "\n",
    "def generate_antonyms_api(root):\n",
    "    res = []\n",
    "    for antonyms in root:\n",
    "        sy_1 = find_synonyms(antonyms[0])\n",
    "        sy_2 = find_synonyms(antonyms[1])\n",
    "        for s in sy_1:\n",
    "            ant = find_antonyms(s)\n",
    "            if ant:\n",
    "                for a in ant:\n",
    "                    word_pair = [s, a]\n",
    "                    res.append(word_pair)\n",
    "        for s in sy_2:\n",
    "            ant = find_antonyms(s)\n",
    "            if ant:\n",
    "                for a in ant:\n",
    "                    word_pair = [s, a]\n",
    "                    res.append(word_pair)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 4min 26s\n"
    }
   ],
   "source": [
    "%%time\n",
    "antonyms_api = generate_antonyms_api(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with generated antonym pairs\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# first method: naive (for generated antonyms from wordnet)\n",
    "def process_antonyms(antonyms, trained_model):\n",
    "    # just reserve the unique word pairs which are contained in trained_model\n",
    "    hash_set = set()\n",
    "    for wordpair in antonyms:\n",
    "        word1, word2 = wordpair\n",
    "        if word1 in trained_model.vocab and word2 in trained_model.vocab:\n",
    "            if word1 > word2:\n",
    "                word1, word2 = word2, word1\n",
    "            hash_set.add((word1, word2))\n",
    "    print(len(hash_set))    \n",
    "    return list(hash_set)\n",
    "\n",
    "#second method: more powerful (for generated antonyms from api)\n",
    "def process_antonyms_max(antonyms, current_model):\n",
    "    hash_set = set()\n",
    "    for wordpair in antonyms:\n",
    "        word1, word2 = wordpair\n",
    "        if word1 in current_model.vocab and word2 in current_model.vocab:\n",
    "            if word1 > word2:\n",
    "                word1, word2 = word2, word1\n",
    "            hash_set.add((word1, word2))\n",
    "    list_antonym = list(hash_set)\n",
    "\n",
    "    similarity_matrix = defaultdict(list)\n",
    "    for each_pair in tqdm(list_antonym):\n",
    "        word1 = each_pair[0]\n",
    "        word2 = each_pair[1]\n",
    "        if word1 < word2:\n",
    "            similarity_matrix[word1].append(word2)\n",
    "        else:\n",
    "            similarity_matrix[word2].append(word1)\n",
    "\n",
    "    all_similarity = defaultdict(dict)\n",
    "    for each_key in tqdm(similarity_matrix):\n",
    "        for each_value in similarity_matrix[each_key]:\n",
    "            all_similarity[each_key][each_value] = abs(cosine_similarity([current_model[each_key]],[current_model[each_value]])[0][0])\n",
    "\n",
    "    final_antonym_list = []\n",
    "    for index_counter, each_key in enumerate(tqdm(all_similarity)):\n",
    "        listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "        final_antonym_list.append((each_key, listofTuples[0][0]))\n",
    "\n",
    "    print(len(final_antonym_list))\n",
    "    return final_antonym_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "35\n36\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=803), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b98e6b527ad4a44a30e2a825c6063a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=435), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5df1ece46f784a3c944aaf15efec1170"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=435), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "faaf88c36a9f4c798e432abdfa6d884c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n435\n"
    }
   ],
   "source": [
    "process_antonyms_root = process_antonyms(root, model_norm)\n",
    "processed_antonyms_wordnet = process_antonyms(antonyms_wordnet, model_norm)\n",
    "processed_antonyms_api = process_antonyms_max(antonyms_api, model_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('portuguese', 'spanish'),\n ('buddhism', 'catholic'),\n ('large', 'small'),\n ('black', 'white'),\n ('desert', 'vegetation'),\n ('english', 'russian'),\n ('chinese', 'english'),\n ('long', 'wide'),\n ('white', 'yellow'),\n ('brown', 'white'),\n ('america', 'russia'),\n ('hindi', 'muslim'),\n ('civilized', 'uncivilized'),\n ('first', 'third'),\n ('america', 'china'),\n ('big', 'tiny'),\n ('catholic', 'muslim'),\n ('hispanic', 'latino'),\n ('city', 'land'),\n ('cosmopolitan', 'limited'),\n ('poor', 'rich'),\n ('development', 'farming'),\n ('busy', 'calm'),\n ('cold', 'hot'),\n ('green', 'grey'),\n ('blue', 'grey'),\n ('asia', 'europa'),\n ('english', 'spanish'),\n ('developed', 'undeveloped'),\n ('china', 'russia'),\n ('baltic', 'scandinavian'),\n ('north', 'south'),\n ('east', 'west'),\n ('modern', 'traditional'),\n ('democratic', 'monarchic')]"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "process_antonyms_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_antonyms(process_antonyms_root, processed_antonyms_wordnet, processed_antonyms_api, current_model):\n",
    "    hash_set = set()\n",
    "    for wordpair in process_antonyms_root:\n",
    "        word1, word2 = wordpair\n",
    "        hash_set.add((word1, word2))\n",
    "    for wordpair in processed_antonyms_wordnet:\n",
    "        word1, word2 = wordpair\n",
    "        hash_set.add((word1, word2))\n",
    "    for wordpair in processed_antonyms_api:\n",
    "        word1, word2 = wordpair\n",
    "        hash_set.add((word1, word2))\n",
    "\n",
    "    list_antonym = list(hash_set)\n",
    "\n",
    "    similarity_matrix = defaultdict(list)\n",
    "    for each_pair in list_antonym:\n",
    "        word1 = each_pair[0]\n",
    "        word2 = each_pair[1]\n",
    "        if word1 < word2:\n",
    "            similarity_matrix[word1].append(word2)\n",
    "        else:\n",
    "            similarity_matrix[word2].append(word1)\n",
    "\n",
    "    all_similarity = defaultdict(dict)\n",
    "    for each_key in similarity_matrix:\n",
    "        for each_value in similarity_matrix[each_key]:\n",
    "            all_similarity[each_key][each_value] = abs(cosine_similarity([current_model[each_key]],[current_model[each_value]])[0][0])\n",
    "\n",
    "    final_antonym_list = []\n",
    "    for index_counter, each_key in enumerate(all_similarity):\n",
    "        listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "        final_antonym_list.append((each_key, listofTuples[0][0]))\n",
    "\n",
    "    print(len(final_antonym_list))\n",
    "    return final_antonym_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "470\n"
    }
   ],
   "source": [
    "final_antonym_list = collect_antonyms(process_antonyms_root, processed_antonyms_wordnet, processed_antonyms_api, model_norm)\n",
    "#store final antonyms\n",
    "# with open('./generated_antonyms/country_related_antonyms', 'w') as f:\n",
    "#     f.writelines([i+' ' +j+'\\n' for i,j in final_antonym_list])\n",
    "with open('./generated_antonyms/country50_related_antonyms.txt', 'w') as f:\n",
    "    f.writelines([i+' ' +j+'\\n' for i,j in final_antonym_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('buddhism', 'catholic'),\n ('descend', 'risen'),\n ('limited', 'universal'),\n ('advancement', 'failure'),\n ('disbelieve', 'meditating'),\n ('disadvantage', 'lead'),\n ('communal', 'personal'),\n ('modern', 'primitive'),\n ('good', 'sinister'),\n ('calm', 'heated'),\n ('broken', 'smooth'),\n ('bitsy', 'giant'),\n ('agitated', 'smooth'),\n ('asia', 'europa'),\n ('burghal', 'suburban'),\n ('deplete', 'feeding'),\n ('cosmopolitan', 'unrefined'),\n ('democratic', 'monarchic'),\n ('embryonic', 'grown'),\n ('exclusive', 'extensive'),\n ('inhabited', 'uninhabited'),\n ('disorderly', 'orderly'),\n ('hurt', 'service'),\n ('desolate', 'full'),\n ('bitty', 'fat'),\n ('civilized', 'unconscionable'),\n ('disturbed', 'placid'),\n ('america', 'russia'),\n ('chaotic', 'orderly'),\n ('city', 'country')]"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "final_antonym_list[:30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}