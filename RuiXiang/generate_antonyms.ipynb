{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate new antonym pairs for specific topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "#loading trained model\n",
    "model_norm = KeyedVectors.load_word2vec_format(\"./Dataset/word2vec_norm\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading original antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_antonym = []\n",
    "current_model = model_norm\n",
    "\n",
    "with open('./Dataset/Antonym_sets/LenciBenotto.val') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "\n",
    "\n",
    "with open('./Dataset/Antonym_sets/LenciBenotto.test') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "with open('./Dataset/Antonym_sets/EVALution.val') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "with open('./Dataset/Antonym_sets/EVALution.test') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "                \n",
    "list_antonym = list(dict.fromkeys(list_antonym).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=2968), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0409c4c5d69b4e2e8a5f66173b006fc8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=1411), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b34316b12644a04b9cc612bdb5cf9e8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=1411), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d760d33d20874dfcb80a494d5e214114"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n1411\n"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = defaultdict(list)\n",
    "for each_pair in tqdm(list_antonym):\n",
    "    word1 = each_pair[0]\n",
    "    word2 = each_pair[1]\n",
    "    if word1 < word2:\n",
    "        similarity_matrix[word1].append(word2)\n",
    "    else:\n",
    "        similarity_matrix[word2].append(word1)\n",
    "    \n",
    "all_similarity = defaultdict(dict)\n",
    "for each_key in tqdm(similarity_matrix):\n",
    "    for each_value in similarity_matrix[each_key]:\n",
    "#         cosine_similarity([current_model[each_key]]\n",
    "        all_similarity[each_key][each_value] = abs(cosine_similarity([current_model[each_key]],[current_model[each_value]])[0][0])\n",
    "    \n",
    "final_antonym_list = []\n",
    "for index_counter, each_key in enumerate(tqdm(all_similarity)):\n",
    "#     print(each_key,all_similarity[each_key])\n",
    "    listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "#     print(listofTuples)\n",
    "    final_antonym_list.append((each_key, listofTuples[0][0]))\n",
    "print(len(final_antonym_list))\n",
    "\n",
    "list_antonym = final_antonym_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store original antonyms\n",
    "with open('./all_antonyms', 'w') as f:\n",
    "    f.writelines([i+' ' +j+'\\n' for i,j in list_antonym])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use original antonyms to find the most similar word pairs related to specific topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "antonyms = {}\n",
    "with open('./all_antonyms', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        word_pair = line.split()\n",
    "        antonyms[i] = word_pair\n",
    "\n",
    "def select_antonym(topic, antonyms, n, model):\n",
    "    similarity_score = defaultdict(float)\n",
    "    for i, words in antonyms.items():\n",
    "        # similarity_score[i] = max(model.similarity(topic,words[0]),model.similarity(topic,words[0]))\n",
    "        similarity_score[i] = model.similarity(topic,words[0]) + model.similarity(topic,words[0])\n",
    "    selected_antonyms = []\n",
    "    sorted_similarity_score = sorted(similarity_score.items(), key = lambda x:x[1], reverse=True)\n",
    "    for item in sorted_similarity_score[:n]:\n",
    "        selected_antonyms.append(antonyms[item[0]])\n",
    "    return selected_antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# generate antonyms for topic 'country'\n",
    "# select_country = select_antonym('country', antonyms, 300, model_norm)\n",
    "# select_china = select_antonym('china', antonyms, 300, model_norm)\n",
    "# select_usa = select_antonym('germany', antonyms, 200, model_norm)\n",
    "# re = []\n",
    "# for i in select_country:\n",
    "#     if i in select_china:\n",
    "#         re.append(i)\n",
    "#store selectd antonyms\n",
    "# with open('./generated_antonyms/CountryWordPairsByHand','w') as f:\n",
    "#     f.writelines(i+' '+j +'\\n' for i,j in re)\n",
    "\n",
    "# for i in select_country:\n",
    "#     if i in select_usa and i not in re:\n",
    "#         print(i[0],i[1])\n",
    "########################################################################\n",
    "\n",
    "# generate antonyms for topic 'food'\n",
    "# select_food = select_antonym('food', antonyms, 80, model_norm)\n",
    "# re = []\n",
    "# for i in select_food:\n",
    "#     re.append(i)\n",
    "# store selectd antonyms\n",
    "# with open('./generated_antonyms/FoodWordPairsByHand','w') as f:\n",
    "#     f.writelines(i+' '+j +'\\n' for i,j in re)\n",
    "#########################################################################\n",
    "\n",
    "# generate antonyms for topic 'music'\n",
    "select_food = select_antonym('music', antonyms, 80, model_norm)\n",
    "re = []\n",
    "for i in select_food:\n",
    "    re.append(i)\n",
    "# store selectd antonyms\n",
    "with open('./generated_antonyms/MusicWordPairsByHand','w') as f:\n",
    "    f.writelines(i+' '+j +'\\n' for i,j in re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use selected antonyms as root to generate new antonyms by Wordnet and API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load root antonyms\n",
    "\n",
    "\n",
    "#load food\n",
    "# with open('./generated_antonyms/FoodWordPairsByHand','r') as f:\n",
    "#     root = [line.strip('\\n').split(' ') for line in f.readlines()]\n",
    "\n",
    "######################################################################\n",
    "#load country\n",
    "# with open('./generated_antonyms/CountryWordPairsByHand','r') as f:\n",
    "#     root = [line.strip('\\n').split(' ') for line in f.readlines()]\n",
    "# with open('./generated_antonyms/country50_by_Jan.txt','r') as f:\n",
    "#     root = [line.strip('\\n').split(' ') for line in f.readlines()]\n",
    "######################################################################\n",
    "\n",
    "#load music\n",
    "with open('./generated_antonyms/MusicWordPairsByHand','r') as f:\n",
    "    root = [line.strip('\\n').split(' ') for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['music', 'quiet'],\n ['dance', 'stand'],\n ['blues', 'folk'],\n ['jazz', 'pop'],\n ['quiet', 'loud'],\n ['popular', 'obsolete'],\n ['modern', 'antique'],\n ['musician', 'mute'],\n ['song', 'talk'],\n ['lyric', 'tune'],\n ['hop', 'jump'],\n ['artist', 'plebeian'],\n ['piano', 'trombone'],\n ['sing', 'talk'],\n ['art', 'ugly'],\n ['rock', 'classic'],\n ['sound', 'touch'],\n ['dissonant', 'symphonic'],\n ['love', 'war'],\n ['audience', 'presentment'],\n ['harmony', 'rivalry'],\n ['blandness', 'sensory'],\n ['radio', 'tv'],\n ['drama', 'parody'],\n ['solo', 'symphonic'],\n ['flute', 'trombone'],\n ['dream', 'real'],\n ['audio', 'leaflet'],\n ['breathless', 'pulmonary'],\n ['joy', 'pain'],\n ['critic', 'musician'],\n ['funny', 'boring'],\n ['cd', 'record'],\n ['spirituality', 'taxonomic'],\n ['resonate', 'single'],\n ['beautiful', 'nasty'],\n ['praise', 'slam'],\n ['beat', 'strum'],\n ['trombone', 'trumpet'],\n ['edit', 'spoil'],\n ['documentary', 'parody'],\n ['potpourri', 'skunk'],\n ['fashion', 'taste'],\n ['enliven', 'immobilize'],\n ['hear', 'taste'],\n ['poetic', 'rambling'],\n ['happy', 'sad'],\n ['ecstatic', 'inconsolable'],\n ['parody', 'truth'],\n ['detest', 'fancy'],\n ['record', 'tape'],\n ['book', 'movie'],\n ['happy', 'mean'],\n ['adult', 'child'],\n ['man', 'woman'],\n ['talk', 'write'],\n ['instrument', 'human'],\n ['brass', 'silver'],\n ['moan', 'screech']]"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case cell\n",
    "#########################\n",
    "# from nltk.corpus import wordnet as wn\n",
    "# wordsets = wn.synsets('good')\n",
    "# for word in wordsets:\n",
    "#     if word.hyponyms():\n",
    "#         hypon = word.hyponyms()\n",
    "#         for h in hypon:\n",
    "#             for w in h.lemmas():\n",
    "#                 if w.antonyms():\n",
    "#                     print(w.name(), w.antonyms()[0].name())\n",
    "# re = [sets.name().split('.')[0] for sets in wordsets]\n",
    "# print(sets.lemmas()[0].antonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate antonyms from wordnet\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def generate_antonyms_wordnet(root):\n",
    "    res = []\n",
    "    for antonyms in root:\n",
    "        #generate antonym pair from synonyms\n",
    "        wordsets = wn.synsets(antonyms[0])\n",
    "        for sets in wordsets:\n",
    "            for w in sets.lemmas():\n",
    "                word1 = w.name()\n",
    "                if w.antonyms():\n",
    "                    word2 = w.antonyms()[0].name()\n",
    "                    word_pair = [word1, word2]\n",
    "                    if word_pair not in res:\n",
    "                        res.append(word_pair)\n",
    "        wordsets = wn.synsets(antonyms[1])\n",
    "        for sets in wordsets:\n",
    "            for w in sets.lemmas():\n",
    "                word1 = w.name()\n",
    "                if w.antonyms():\n",
    "                    word2 = w.antonyms()[0].name()\n",
    "                    word_pair = [word1, word2]\n",
    "                    if word_pair not in res:\n",
    "                        res.append(word_pair)\n",
    "        #generate antonym pair from hyponyms\n",
    "        wordsets = wn.synsets(antonyms[0])\n",
    "        for sets in wordsets:\n",
    "            if sets.hyponyms():\n",
    "                hypon = sets.hyponyms()\n",
    "                for h in hypon:\n",
    "                    for w in h.lemmas():\n",
    "                        word1 = w.name()\n",
    "                        if w.antonyms():\n",
    "                            word2 = w.antonyms()[0].name()\n",
    "                            word_pair = [word1, word2]\n",
    "                            if word_pair not in res:\n",
    "                                 res.append(word_pair)\n",
    "        wordsets = wn.synsets(antonyms[1])\n",
    "        for sets in wordsets:\n",
    "            if sets.hyponyms():\n",
    "                hypon = sets.hyponyms()\n",
    "                for h in hypon:\n",
    "                    for w in h.lemmas():\n",
    "                        word1 = w.name()\n",
    "                        if w.antonyms():\n",
    "                            word2 = w.antonyms()[0].name()\n",
    "                            word_pair = [word1, word2]\n",
    "                            if word_pair not in res:\n",
    "                                res.append(word_pair)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "antonyms_wordnet = generate_antonyms_wordnet(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate antonyms from API: http://www.thesaurus.com\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def find_synonyms(term):\n",
    "    response = requests.get('http://www.thesaurus.com/browse/{}'.format(term))\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    section = soup.find('ul', {'class': 'css-17d6qyx-WordGridLayoutBox et6tpn80'})\n",
    "    if section:\n",
    "        # find 10 synonyms\n",
    "        return [li.text for li in section.findAll('li')][:10]\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def find_antonyms(term):\n",
    "    response = requests.get('http://www.thesaurus.com/browse/{}'.format(term))\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    if soup('ul', {'class': 'css-17d6qyx-WordGridLayoutBox et6tpn80'}):\n",
    "        soup('ul', {'class': 'css-17d6qyx-WordGridLayoutBox et6tpn80'})[0].extract()\n",
    "        section = soup.find('ul', {'class': 'css-17d6qyx-WordGridLayoutBox et6tpn80'})\n",
    "        if section:\n",
    "            # find 3 antonyms\n",
    "            return [li.text for li in section.findAll('li')][:3]\n",
    "        else:\n",
    "            return \n",
    "\n",
    "def generate_antonyms_api(root):\n",
    "    res = []\n",
    "    for antonyms in root:\n",
    "        sy_1 = find_synonyms(antonyms[0])\n",
    "        sy_2 = find_synonyms(antonyms[1])\n",
    "        for s in sy_1:\n",
    "            ant = find_antonyms(s)\n",
    "            if ant:\n",
    "                for a in ant:\n",
    "                    word_pair = [s, a]\n",
    "                    res.append(word_pair)\n",
    "        for s in sy_2:\n",
    "            ant = find_antonyms(s)\n",
    "            if ant:\n",
    "                for a in ant:\n",
    "                    word_pair = [s, a]\n",
    "                    res.append(word_pair)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 8min 28s\n"
    }
   ],
   "source": [
    "%%time\n",
    "antonyms_api = generate_antonyms_api(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with generated antonym pairs\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# first method: naive (for generated antonyms from wordnet)\n",
    "def process_antonyms(antonyms, trained_model):\n",
    "    # just reserve the unique word pairs which are contained in trained_model\n",
    "    hash_set = set()\n",
    "    for wordpair in antonyms:\n",
    "        word1, word2 = wordpair\n",
    "        if word1 in trained_model.vocab and word2 in trained_model.vocab:\n",
    "            if word1 > word2:\n",
    "                word1, word2 = word2, word1\n",
    "            hash_set.add((word1, word2))\n",
    "    print(len(hash_set))    \n",
    "    return list(hash_set)\n",
    "\n",
    "#second method: more powerful (for generated antonyms from api)\n",
    "def process_antonyms_max(antonyms, current_model):\n",
    "    hash_set = set()\n",
    "    for wordpair in antonyms:\n",
    "        word1, word2 = wordpair\n",
    "        if word1 in current_model.vocab and word2 in current_model.vocab:\n",
    "            if word1 > word2:\n",
    "                word1, word2 = word2, word1\n",
    "            hash_set.add((word1, word2))\n",
    "    list_antonym = list(hash_set)\n",
    "\n",
    "    similarity_matrix = defaultdict(list)\n",
    "    for each_pair in tqdm(list_antonym):\n",
    "        word1 = each_pair[0]\n",
    "        word2 = each_pair[1]\n",
    "        if word1 < word2:\n",
    "            similarity_matrix[word1].append(word2)\n",
    "        else:\n",
    "            similarity_matrix[word2].append(word1)\n",
    "\n",
    "    all_similarity = defaultdict(dict)\n",
    "    for each_key in tqdm(similarity_matrix):\n",
    "        for each_value in similarity_matrix[each_key]:\n",
    "            all_similarity[each_key][each_value] = abs(cosine_similarity([current_model[each_key]],[current_model[each_value]])[0][0])\n",
    "\n",
    "    final_antonym_list = []\n",
    "    for index_counter, each_key in enumerate(tqdm(all_similarity)):\n",
    "        listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "        final_antonym_list.append((each_key, listofTuples[0][0]))\n",
    "\n",
    "    print(len(final_antonym_list))\n",
    "    return final_antonym_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "58\n67\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=1557), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1899997d8b724234a34cf6edbd11f193"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=773), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23d2a6d499f9472d96b7f9b074d8be67"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=773), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75f3db13b72f42339a6d91f384f81ac1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n773\n"
    }
   ],
   "source": [
    "process_antonyms_root = process_antonyms(root, model_norm)\n",
    "processed_antonyms_wordnet = process_antonyms(antonyms_wordnet, model_norm)\n",
    "processed_antonyms_api = process_antonyms_max(antonyms_api, model_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('harmony', 'rivalry'),\n ('blandness', 'sensory'),\n ('music', 'quiet'),\n ('brass', 'silver'),\n ('happy', 'mean'),\n ('jazz', 'pop'),\n ('parody', 'truth'),\n ('audio', 'leaflet'),\n ('spirituality', 'taxonomic'),\n ('adult', 'child'),\n ('drama', 'parody'),\n ('record', 'tape'),\n ('artist', 'plebeian'),\n ('antique', 'modern'),\n ('enliven', 'immobilize'),\n ('happy', 'sad'),\n ('hop', 'jump'),\n ('joy', 'pain'),\n ('talk', 'write'),\n ('dream', 'real'),\n ('song', 'talk'),\n ('lyric', 'tune'),\n ('audience', 'presentment'),\n ('fashion', 'taste'),\n ('loud', 'quiet'),\n ('radio', 'tv'),\n ('documentary', 'parody'),\n ('ecstatic', 'inconsolable'),\n ('hear', 'taste'),\n ('man', 'woman'),\n ('love', 'war'),\n ('detest', 'fancy'),\n ('praise', 'slam'),\n ('classic', 'rock'),\n ('critic', 'musician'),\n ('dissonant', 'symphonic'),\n ('cd', 'record'),\n ('beautiful', 'nasty'),\n ('obsolete', 'popular'),\n ('human', 'instrument'),\n ('potpourri', 'skunk'),\n ('sing', 'talk'),\n ('flute', 'trombone'),\n ('book', 'movie'),\n ('boring', 'funny'),\n ('dance', 'stand'),\n ('art', 'ugly'),\n ('musician', 'mute'),\n ('trombone', 'trumpet'),\n ('resonate', 'single'),\n ('beat', 'strum'),\n ('poetic', 'rambling'),\n ('moan', 'screech'),\n ('piano', 'trombone'),\n ('edit', 'spoil'),\n ('sound', 'touch'),\n ('solo', 'symphonic'),\n ('breathless', 'pulmonary')]"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "process_antonyms_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_antonyms(process_antonyms_root, processed_antonyms_wordnet, processed_antonyms_api, current_model):\n",
    "    hash_set = set()\n",
    "    for wordpair in process_antonyms_root:\n",
    "        word1, word2 = wordpair\n",
    "        hash_set.add((word1, word2))\n",
    "    for wordpair in processed_antonyms_wordnet:\n",
    "        word1, word2 = wordpair\n",
    "        hash_set.add((word1, word2))\n",
    "    for wordpair in processed_antonyms_api:\n",
    "        word1, word2 = wordpair\n",
    "        hash_set.add((word1, word2))\n",
    "\n",
    "    list_antonym = list(hash_set)\n",
    "\n",
    "    similarity_matrix = defaultdict(list)\n",
    "    for each_pair in list_antonym:\n",
    "        word1 = each_pair[0]\n",
    "        word2 = each_pair[1]\n",
    "        if word1 < word2:\n",
    "            similarity_matrix[word1].append(word2)\n",
    "        else:\n",
    "            similarity_matrix[word2].append(word1)\n",
    "\n",
    "    all_similarity = defaultdict(dict)\n",
    "    for each_key in similarity_matrix:\n",
    "        for each_value in similarity_matrix[each_key]:\n",
    "            all_similarity[each_key][each_value] = abs(cosine_similarity([current_model[each_key]],[current_model[each_value]])[0][0])\n",
    "\n",
    "    final_antonym_list = []\n",
    "    for index_counter, each_key in enumerate(all_similarity):\n",
    "        listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "        final_antonym_list.append((each_key, listofTuples[0][0]))\n",
    "\n",
    "    print(len(final_antonym_list))\n",
    "    return final_antonym_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "858\n"
    }
   ],
   "source": [
    "final_antonym_list = collect_antonyms(process_antonyms_root, processed_antonyms_wordnet, processed_antonyms_api, model_norm)\n",
    "#store final antonyms\n",
    "\n",
    "##########################################################################\n",
    "#store food\n",
    "# with open('./generated_antonyms/food_related_antonyms', 'w') as f:\n",
    "#     f.writelines([i+' ' +j+'\\n' for i,j in final_antonym_list])\n",
    "##########################################################################\n",
    "#store country\n",
    "# with open('./generated_antonyms/country_related_antonyms', 'w') as f:\n",
    "#     f.writelines([i+' ' +j+'\\n' for i,j in final_antonym_list])\n",
    "# with open('./generated_antonyms/country50_related_antonyms.txt', 'w') as f:\n",
    "#     f.writelines([i+' ' +j+'\\n' for i,j in final_antonym_list])\n",
    "############################################################################\n",
    "#store music\n",
    "with open('./generated_antonyms/music_related_antonyms', 'w') as f:\n",
    "    f.writelines([i+' ' +j+'\\n' for i,j in final_antonym_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('beautify', 'mangle'),\n ('arrange', 'disorder'),\n ('frilly', 'heavy'),\n ('horrid', 'pleasant'),\n ('generous', 'mercenary'),\n ('mature', 'silly'),\n ('benevolence', 'gall'),\n ('abstain', 'choose'),\n ('husband', 'wife'),\n ('blame', 'compliment'),\n ('idea', 'reality'),\n ('damaged', 'sound'),\n ('joy', 'pain'),\n ('low', 'raucous'),\n ('general', 'solitary'),\n ('congregation', 'division'),\n ('baby', 'enormous'),\n ('censor', 'expand'),\n ('cheek', 'meekness'),\n ('giving', 'miserly'),\n ('contact', 'division'),\n ('dislike', 'harmony'),\n ('daydream', 'dislike'),\n ('household', 'special'),\n ('brother', 'sister'),\n ('glad', 'upset'),\n ('connected', 'incoherent'),\n ('nominal', 'real'),\n ('imbalanced', 'stable'),\n ('cramp', 'ease')]"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "final_antonym_list[:30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}