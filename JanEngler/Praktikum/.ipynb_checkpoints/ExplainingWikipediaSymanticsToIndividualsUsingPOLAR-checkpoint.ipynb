{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forschungspraktikum SS20 \n",
    "# Semantic Differentials for Wikipedia using the POLAR Framework\n",
    "\n",
    "\n",
    "The POLAR Framework is a method that enables Interpretability for pre-trained word embeddings. The goal of this project is to produce word embeddings from a Wikipedia dataset, and deploy the POLAR framework to different categories of words (e.g. Countries, Politicians, Music, etc) in order to explain the semantic associations behind concepts on Wikipedia. An evaluation setup should assess the quality of the semantic differentials produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from gensim.corpora.wikicorpus import WikiCorpus #https://radimrehurek.com/gensim/corpora/wikicorpus.html\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only run next cell if a new Wikipedia dataset is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNewModel=False\n",
    "loadNewDataset= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trainNewModel==True:\n",
    "    wiki = WikiCorpus(r\"C:\\Users\\Jan\\Desktop\\PraktikumSS20\\Semantic-Differentials-for-Wikipedia-using-the-POLAR-Framework\\JanEngler\\Praktikum\\enwiki-20200401-pages-articles-multistream1.xml-p1p30303.bz2\", \n",
    "                  lemmatize=False, dictionary={})\n",
    "    #stores the preprocessed dataset, which reduces runtime for the future\n",
    "    wiki.save('SavedWord2Vec/wiki.corpus')\n",
    "    \n",
    "    sentences = list(wiki.get_texts()) #Iterate over the dump, yielding a list of tokens for each article that passed the length and namespace filtering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "size: The number of dimensions of the embeddings and the default is 100.\n",
    "\n",
    "window: The maximum distance between a target word and words around the target word. The default window is 5.\n",
    "\n",
    "min_count: The minimum count of words to consider when training the model; words with occurrence less than this \n",
    "count will be ignored. The default for min_count is 5.\n",
    "\n",
    "workers: The number of partitions during training and the default workers is 3.\n",
    "\n",
    "sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model from file...\n",
      "Finished Loading\n"
     ]
    }
   ],
   "source": [
    "### Compute the Word2Vec model usint following parameters#\n",
    "# Currently using CBOW for testing because of performance issues of skip gram on my laptop #\n",
    "if trainNewModel==True:\n",
    "    print(\"Training Word2Vec model ...\")\n",
    "    params = {'size': 300, 'window': 5, 'min_count': 5, \n",
    "          'workers': max(1, multiprocessing.cpu_count() - 1), 'sample': 1E-3,'sg' : 1}\n",
    "    word2vec = Word2Vec(sentences, **params)\n",
    "    ### Save Word2Vec model to file ###\n",
    "    word2vec.save('SavedWord2Vec/wiki.word2vec.model')\n",
    "    print(\"Finished Training\")\n",
    "else:\n",
    "    ### Loading finished Word2Vec model to memory from file ###\n",
    "    print(\"Loading Word2Vec model from file...\")\n",
    "    word2vec = Word2Vec.load('SavedWord2Vec/wiki.word2vec.model')\n",
    "    print(\"Finished Loading\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. griffiths (1.49)\n",
      "2. earle (1.49)\n",
      "3. lindsay (1.49)\n",
      "4. lester (1.48)\n",
      "5. melania (1.47)\n"
     ]
    }
   ],
   "source": [
    "female_king = word2vec.most_similar_cosmul(positive='donald'.split(), \n",
    "                                           negative='mtu'.split(), topn=5,)\n",
    "for ii, (word, score) in enumerate(female_king):\n",
    "    print(\"{}. {} ({:1.2f})\".format(ii+1, word, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58086324\n"
     ]
    }
   ],
   "source": [
    "def similarityNameVector(vec, name):\n",
    "    \n",
    "    cosineSimilarity = np.dot(vec, word2vec.wv[name])/(np.linalg.norm(vec)* np.linalg.norm(word2vec.wv[name]))\n",
    "    return cosineSimilarity\n",
    "\n",
    "print(similarityNameVector(ourQueen,'queen'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smiliarity of king and queen: 0.61206675\n",
      "Smiliarity of transformed king and queen: 0.58086324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('king', 0.7404661178588867),\n",
       " ('queen', 0.5808632373809814),\n",
       " ('regnant', 0.5323577523231506),\n",
       " ('woman', 0.5306766629219055),\n",
       " ('consort', 0.4988700747489929),\n",
       " ('sibylla', 0.4976164400577545),\n",
       " ('monarch', 0.49684637784957886),\n",
       " ('berengaria', 0.4891374111175537),\n",
       " ('husband', 0.48792698979377747),\n",
       " ('isabeau', 0.4850718677043915)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Smiliarity of king and queen:\", word2vec.wv.similarity('king', 'queen'))\n",
    "ourQueen= word2vec.wv['king'] - word2vec.wv['man'] + word2vec.wv['woman']\n",
    "print(\"Smiliarity of transformed king and queen:\", similarityNameVector(ourQueen,'queen'))\n",
    "\n",
    "word2vec.wv.most_similar(positive=[ourQueen], topn=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52884626\n",
      "0.23710738\n"
     ]
    }
   ],
   "source": [
    "print(word2vec.wv.similarity('germany', 'europe')) \n",
    "print(word2vec.wv.similarity('leyen', 'europe'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, Euclidian similarity cannot work well for the high-dimensional word vectors.\n",
    "This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings.\n",
    "Alternatively, we can use cosine similarity to measure the similarity between two vectors.\n",
    "Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41214263"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.similarity('star', 'hollywood')  \n",
    "#'k√∂ln', 'akademie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('afghan', 0.7354598641395569),\n",
       " ('iran', 0.7224469184875488),\n",
       " ('kabul', 0.714802086353302),\n",
       " ('iraq', 0.7087748050689697),\n",
       " ('pakistan', 0.6993895769119263),\n",
       " ('taliban', 0.6594675779342651),\n",
       " ('tajikistan', 0.6579927206039429),\n",
       " ('herat', 0.6495062708854675),\n",
       " ('kargil', 0.6288100481033325),\n",
       " ('kandahar', 0.6270360946655273)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar('afghanistan')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "def display_closestwords_tsnescatterplot(model, word, size):\n",
    "    \n",
    "    arr = np.empty((0,size), dtype='f')\n",
    "    word_labels = [word]\n",
    "    close_words = model.wv.similar_by_word(word)\n",
    "    #print(arr)\n",
    "    #print(np.array([model[word]]))\n",
    "    arr = np.append(arr, np.array([model.wv[word]]), axis=0)\n",
    "    \n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "        plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "        plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "display_closestwords_tsnescatterplot(word2vec, 'trump', 300) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online training / Resuming training\n",
    "\n",
    "Advanced users can load a model and continue training it with more sentences and new vocabulary words:\n",
    "\n",
    "model = gensim.models.Word2Vec.load(temporary_filepath)\n",
    "more_sentences = [\n",
    "    ['Advanced', 'users', 'can', 'load', 'a', 'model',\n",
    "     'and', 'continue', 'training', 'it', 'with', 'more', 'sentences']\n",
    "]\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "\n",
    "cleaning up temporary file\n",
    "import os\n",
    "os.remove(temporary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from numpy import linalg\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "from random import shuffle\n",
    "import sys\n",
    "import nltk \n",
    "from nltk.corpus import wordnet \n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "#%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model=model = word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Jan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\Users\\Jan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\Users\\Jan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n"
     ]
    }
   ],
   "source": [
    "list_antonym = []\n",
    "\n",
    "with open(r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Antonym_sets\\LenciBenotto.val') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "\n",
    "\n",
    "with open(r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Antonym_sets\\LenciBenotto.test') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "with open(r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Antonym_sets\\EVALution.val') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "with open(r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Antonym_sets\\EVALution.test') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "                \n",
    "list_antonym = list(dict.fromkeys(list_antonym).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('implicate', 'exclude'), ('assertion', 'rejection'), ('terminal', 'beginning'), ('lop', 'secure'), ('pastor', 'libertine'), ('pocket', 'hook'), ('orbital', 'stationary'), ('evidence', 'disprove'), ('psychological', 'biological'), ('preparedness', 'unprepared'), ('saturation', 'dehydration'), ('geologic', 'biologic'), ('accessibility', 'secured'), ('fight', 'agree'), ('lump', 'smooth'), ('double', 'half'), ('training', 'forgetting'), ('determined', 'negligent'), ('velocity', 'still'), ('shield', 'vulnerable'), ('measurement', 'guess'), ('iconic', 'ordinary'), ('thump', 'caress'), ('stretcher', 'compressor'), ('allude', 'distract'), ('cemetery', 'birthplace'), ('corrupt', 'sanctify'), ('cameraman', 'actor'), ('knowledgeable', 'uninformed'), ('screech', 'talk'), ('hedge', 'squander'), ('aggregate', 'please'), ('constrict', 'open'), ('make', 'break'), ('normative', 'descriptive'), ('inscribe', 'polish'), ('bulge', 'hollow'), ('edit', 'expand'), ('sensory', 'numb'), ('immobilize', 'move'), ('iodine', 'water'), ('innovator', 'follower'), ('arachnid', 'serpent'), ('coupling', 'divorce'), ('zone', 'free'), ('caretaker', 'stranger'), ('admiration', 'loathing'), ('pneumatic', 'clear'), ('amalgamate', 'separate'), ('arbor', 'wasteland')]\n",
      "2955\n"
     ]
    }
   ],
   "source": [
    "print(list_antonym[0:50])\n",
    "print(len(list_antonym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b899fe95ed9f4a82a785e1eefbc9998e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2955), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78530ffcd27a45c88d7c9a2e6bdbb2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1409), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d385b8e49994d2e8a9b3b1c29552539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1409), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('exclude', 'inscribe'), ('assertion', 'lazy'), ('beginning', 'terminal'), ('lop', 'secure'), ('libertine', 'pastor'), ('hook', 'pocket'), ('orbital', 'stable'), ('disprove', 'document'), ('biological', 'psychological'), ('preparedness', 'unplanned'), ('dehydration', 'saturation'), ('biologic', 'geologic'), ('accessibility', 'secured'), ('agree', 'crab'), ('lump', 'spread'), ('double', 'indivisible'), ('forgetting', 'training'), ('determined', 'lazy'), ('still', 'velocity'), ('shield', 'vulnerable'), ('guess', 'measurement'), ('iconic', 'ordinary'), ('caress', 'punch'), ('compressor', 'stretcher'), ('allude', 'declare'), ('birthplace', 'cemetery'), ('corrupt', 'honor'), ('actor', 'cameraman'), ('knowledgeable', 'naive'), ('screech', 'talk'), ('hedge', 'plain'), ('aggregate', 'please'), ('constrict', 'release'), ('break', 'connect'), ('descriptive', 'normative'), ('inscribe', 'polish'), ('bulge', 'compress'), ('edit', 'include'), ('numb', 'sensory'), ('immobilize', 'tour'), ('iodine', 'water'), ('follower', 'leader'), ('arachnid', 'fly'), ('coupling', 'solitude'), ('free', 'stockade'), ('caretaker', 'continual'), ('admiration', 'hate'), ('clear', 'thunder'), ('amalgamate', 'parse'), ('arbor', 'barren'), ('arboreal', 'urban'), ('disconnection', 'intercourse'), ('journey', 'remain'), ('decentralize', 'streamline'), ('concise', 'redundant'), ('brush', 'hedge'), ('disperse', 'lump'), ('abuser', 'caretaker'), ('boredom', 'party'), ('confer', 'retrieve'), ('cow', 'lobster'), ('metamorphosis', 'unchanged'), ('kiwi', 'vegetable'), ('blueberry', 'pea'), ('mystical', 'normal'), ('absolve', 'bill'), ('initial', 'terminal'), ('fan', 'stifle'), ('defamation', 'promotional'), ('philosophical', 'unreasonable'), ('flick', 'punch'), ('cameraman', 'subject'), ('genuine', 'superficial'), ('picture', 'write'), ('poor', 'shah'), ('bragging', 'humble'), ('cemetery', 'nursery'), ('appealing', 'gross'), ('argue', 'confer'), ('muddle', 'organise'), ('accountant', 'musician'), ('crazy', 'rationale'), ('rest', 'tire'), ('metropolitan', 'rural'), ('civic', 'external'), ('abstain', 'try'), ('cure', 'diagnose'), ('copycat', 'innovator'), ('assailant', 'caretaker'), ('porcupine', 'zebra'), ('humble', 'owed'), ('factual', 'gauge'), ('spiral', 'straighten'), ('experience', 'new'), ('physical', 'psychiatric'), ('complicate', 'plain'), ('sleet', 'snowball'), ('orange', 'tomato'), ('boom', 'decay'), ('groove', 'invalidate'), ('citation', 'invisible'), ('fatalistic', 'open'), ('chemical', 'sweet'), ('contaminate', 'purity'), ('disciplinary', 'free'), ('confine', 'release'), ('caffeine', 'marijuana'), ('pneumatic', 'shapeless'), ('interpret', 'misinterpret'), ('auditory', 'deaf'), ('construe', 'keep'), ('condemn', 'pardon'), ('consume', 'power'), ('congestive', 'digestive'), ('bird', 'human'), ('clam', 'open'), ('deconstruct', 'structure'), ('leaflet', 'novel'), ('conjunct', 'esoteric'), ('disclose', 'secrete'), ('presentment', 'surprise'), ('dim', 'twinkle'), ('abuse', 'worship'), ('aware', 'naivety'), ('linguistic', 'silent'), ('estimate', 'gauge'), ('end', 'spring'), ('distasteful', 'sensory'), ('doing', 'training'), ('erase', 'tabulate'), ('condenser', 'stretcher'), ('incandescent', 'tallow'), ('mislead', 'redirect'), ('countryside', 'township'), ('gauge', 'ignore'), ('critique', 'guess'), ('settle', 'tour'), ('conceal', 'disclose'), ('directionless', 'helm'), ('anarchist', 'shah'), ('heal', 'kill'), ('fixer', 'wrecker'), ('polish', 'rough'), ('avoid', 'tour'), ('abdicate', 'challenge'), ('barley', 'weed'), ('fiscal', 'unable'), ('atmospheric', 'geologic'), ('supervised', 'unsupervised'), ('affix', 'lop'), ('open', 'zone'), ('thump', 'whine'), ('endemic', 'foreign'), ('dodge', 'inscribe'), ('catalogue', 'disorder'), ('obituary', 'parody'), ('naivety', 'wise'), ('audition', 'avoid'), ('planet', 'satellite'), ('intoxicate', 'sober'), ('emotional', 'reserved'), ('correct', 'corrupt'), ('cole', 'mineral'), ('flower', 'skunk'), ('coil', 'spring'), ('able', 'invalid'), ('forget', 'pine'), ('muscular', 'neural'), ('boastful', 'humble'), ('chair', 'table'), ('despotic', 'fair'), ('digestive', 'unhealthy'), ('calculated', 'pneumatic'), ('dirty', 'evidence'), ('stout', 'thin'), ('distress', 'groove'), ('artist', 'plebeian'), ('better', 'corrupt'), ('automated', 'manual'), ('dumb', 'philosophical'), ('specific', 'vague'), ('court', 'repel'), ('insured', 'uninsured'), ('coriander', 'none'), ('chap', 'chick'), ('crab', 'joy'), ('meaningless', 'symbolism'), ('thankful', 'ungrateful'), ('hurricane', 'sandstorm'), ('fancy', 'plain'), ('concealment', 'presentment'), ('normal', 'psychiatric'), ('emptiness', 'saturation'), ('lean', 'stout'), ('randomize', 'tabulate'), ('compass', 'lost'), ('invade', 'protect'), ('cleanse', 'corrupt'), ('dull', 'polish'), ('innovator', 'traditionalist'), ('awaken', 'tire'), ('straightforward', 'symbolic'), ('buff', 'knitwear'), ('processed', 'unrefined'), ('potpourri', 'skunk'), ('exoteric', 'mystical'), ('historic', 'insignificant'), ('fashion', 'taste'), ('innate', 'training'), ('pardon', 'punish'), ('fish', 'mammal'), ('terminal', 'unfinished'), ('slider', 'stationary'), ('extrasensory', 'sensory'), ('divide', 'sum'), ('complete', 'piece'), ('immortal', 'mortal'), ('emperor', 'peasant'), ('advantageous', 'disadvantageous'), ('unsupervised', 'watched'), ('tribal', 'universal'), ('cod', 'shark'), ('estimation', 'measurement'), ('pasture', 'stockade'), ('resting', 'training'), ('pristine', 'unclean'), ('boring', 'climatic'), ('park', 'ride'), ('ignore', 'nose'), ('bop', 'sit'), ('prime', 'worst'), ('dust', 'stone'), ('pulmonary', 'systemic'), ('fact', 'lie'), ('chastity', 'sexuality'), ('create', 'kill'), ('mantle', 'window'), ('current', 'historic'), ('antimatter', 'matter'), ('instructive', 'unclear'), ('halter', 'release'), ('grape', 'kiwi'), ('nationality', 'solitude'), ('aircraft', 'car'), ('chicken', 'turkey'), ('beeline', 'zigzag'), ('inhabit', 'vacate'), ('gross', 'polite'), ('loner', 'tribal'), ('lawful', 'unlawful'), ('disjoint', 'joint'), ('birth', 'die'), ('unite', 'zone'), ('confirmed', 'unconfirmed'), ('soft', 'stone'), ('accidental', 'intentional'), ('invalid', 'true'), ('invalidate', 'support'), ('pad', 'stone'), ('combine', 'tear'), ('invent', 'replicate'), ('spring', 'squat'), ('unicorn', 'zebra'), ('beat', 'strum'), ('implicate', 'save'), ('encrypt', 'interpret'), ('automatic', 'manual'), ('irrational', 'rationale'), ('confuse', 'structure'), ('cooperation', 'rivalry'), ('neglect', 'training'), ('consist', 'undo'), ('partial', 'whole'), ('disadvantageous', 'good'), ('sink', 'uplift'), ('moan', 'screech'), ('irregularly', 'quarterly'), ('denial', 'psychology'), ('deadpan', 'emotional'), ('block', 'fan'), ('matter', 'nothing'), ('confusing', 'instructive'), ('quit', 'try'), ('drop', 'pyramid'), ('fungus', 'iodine'), ('pastime', 'toil'), ('blind', 'corneal'), ('educator', 'liar'), ('copier', 'innovator'), ('export', 'import'), ('rationale', 'senseless'), ('disassemble', 'edit'), ('hiding', 'presentment'), ('exceptional', 'modal'), ('alleviation', 'starting'), ('carnivore', 'omnivore'), ('ground', 'skyline'), ('skunk', 'tie'), ('annual', 'quarterly'), ('start', 'terminal'), ('tap', 'thump'), ('airless', 'pneumatic'), ('country', 'metropolitan'), ('grow', 'trim'), ('pine', 'progress'), ('commitment', 'detachment'), ('impossible', 'probable'), ('chat', 'presentment'), ('leaf', 'leaflet'), ('evidence', 'miss'), ('impure', 'pure'), ('intercourse', 'isolation'), ('dependent', 'unsupervised'), ('known', 'unknown'), ('ignorance', 'training'), ('excitable', 'tame'), ('slowness', 'velocity'), ('congruence', 'contradiction'), ('cyclonic', 'placid'), ('lose', 'tight'), ('shout', 'whisper'), ('documentary', 'parody'), ('rain', 'sun'), ('flood', 'sandstorm'), ('set', 'slam'), ('scribe', 'talk'), ('solidify', 'whisk'), ('symphonic', 'untuned'), ('convict', 'pardon'), ('harmful', 'pathological'), ('hole', 'pocket'), ('indeterminate', 'nationality'), ('flask', 'vat'), ('enable', 'immobilize'), ('guide', 'swat'), ('psychiatric', 'well'), ('balanced', 'psychiatric'), ('deduce', 'scramble'), ('encore', 'over'), ('deteriorate', 'make'), ('asexuality', 'sexuality'), ('detest', 'fancy'), ('saturation', 'solid'), ('pocket', 'rock'), ('settled', 'tribal'), ('lowest', 'prime'), ('collection', 'distribution'), ('trombone', 'trumpet'), ('discourage', 'uplift'), ('battle', 'treat'), ('opening', 'pocket'), ('give', 'pocket'), ('document', 'forget'), ('shoreline', 'skyline'), ('smooth', 'trouble'), ('agreement', 'contradiction'), ('goodbye', 'hello'), ('rub', 'scratch'), ('affirm', 'evidence'), ('hush', 'pontificate'), ('confident', 'neural'), ('maze', 'peace'), ('ingrate', 'thankful'), ('pathological', 'proper'), ('maximum', 'minimum'), ('idle', 'recreation'), ('bottom', 'halter'), ('eternal', 'mortal'), ('ambiguity', 'commitment'), ('trivial', 'vital'), ('extravagant', 'thrifty'), ('mate', 'split'), ('flock', 'pastor'), ('improbable', 'probable'), ('cerebral', 'emotional'), ('extricate', 'implicate'), ('glare', 'wink'), ('conservative', 'nihilistic'), ('completeness', 'emptiness'), ('desert', 'saloon'), ('pontificate', 'withdraw'), ('fight', 'nice'), ('fee', 'free'), ('bothered', 'unaffected'), ('elephant', 'plant'), ('face', 'hedge'), ('leopard', 'zebra'), ('photographic', 'ugly'), ('aft', 'helm'), ('born', 'uterine'), ('saloon', 'teahouse'), ('locality', 'nationality'), ('exonerate', 'implicate'), ('postal', 'verbal'), ('kill', 'make'), ('quiet', 'talk'), ('annoy', 'rub'), ('queen', 'shah'), ('bible', 'geologic'), ('dry', 'river'), ('purity', 'sexuality'), ('flat', 'hill'), ('republicans', 'workforce'), ('barrel', 'flask'), ('contrast', 'copy'), ('bare', 'shield'), ('repel', 'structure'), ('rivalry', 'unity'), ('careless', 'provident'), ('devolve', 'keep'), ('cloud', 'stone'), ('harmony', 'rivalry'), ('assume', 'evidence'), ('pet', 'swat'), ('oppose', 'trim'), ('promotional', 'reserved'), ('apathetic', 'excitable'), ('mindless', 'philosophical'), ('germ', 'iodine'), ('pedantic', 'poetic'), ('alert', 'silence'), ('individual', 'workforce'), ('audio', 'leaflet'), ('scatter', 'zone'), ('lengthen', 'tabulate'), ('episodic', 'orbital'), ('dead', 'immortal'), ('decline', 'try'), ('nasal', 'uterine'), ('demolish', 'polish'), ('arrival', 'journey'), ('contradiction', 'verification'), ('training', 'untrained'), ('knitwear', 'underwear'), ('petite', 'stout'), ('funded', 'impoverished'), ('meaninglessness', 'symbolism'), ('power', 'weak'), ('stroll', 'tour'), ('prostate', 'uterine'), ('indispensable', 'trash'), ('abstinence', 'intercourse'), ('concentration', 'distribution'), ('medicinal', 'psychology'), ('modal', 'still'), ('compress', 'spring'), ('bump', 'groove'), ('omnivore', 'vegan'), ('lessen', 'snowball'), ('slow', 'whisk'), ('cut', 'power'), ('calm', 'fan'), ('drag', 'whisk'), ('listen', 'pontificate'), ('inferior', 'prime'), ('covered', 'uninsured'), ('forward', 'vague'), ('punch', 'stroke'), ('fall', 'raise'), ('layperson', 'pastor'), ('closure', 'split'), ('explicit', 'vague'), ('democratic', 'despotic'), ('leisure', 'rush'), ('mend', 'split'), ('cluster', 'zone'), ('lower', 'pyramid'), ('organized', 'unstructured'), ('land', 'sky'), ('nihilistic', 'positive'), ('mountainous', 'oceanic'), ('geologic', 'plastic'), ('hopeful', 'nihilistic'), ('dimensional', 'static'), ('church', 'saloon'), ('bill', 'coin'), ('affected', 'unaffected'), ('actual', 'symbolic'), ('singular', 'tribal'), ('morphological', 'spiritual'), ('meaningful', 'nihilistic'), ('praise', 'slam'), ('buy', 'free'), ('satisfactory', 'unsatisfactory'), ('arrive', 'leave'), ('hat', 'mantle'), ('dissonant', 'symphonic'), ('parting', 'rendezvous'), ('sexless', 'sexuality'), ('ink', 'paper'), ('continue', 'redirect'), ('blacken', 'picture'), ('moon', 'planet'), ('dispersion', 'distribution'), ('manual', 'visual'), ('division', 'intercourse'), ('rainstorm', 'sandstorm'), ('deprivation', 'sensory'), ('depleted', 'funded'), ('irregular', 'normative'), ('associative', 'friendly'), ('differentiate', 'replicate'), ('cube', 'pyramid'), ('flatter', 'slam'), ('lavish', 'thrifty'), ('chatter', 'psychology'), ('unconfirmed', 'verified'), ('massive', 'weightless'), ('depressing', 'excitable'), ('chaos', 'compass'), ('tour', 'wander'), ('perfume', 'skunk'), ('question', 'symbolism'), ('rumour', 'truth'), ('audience', 'presentment'), ('recreation', 'work'), ('helm', 'powerlessness'), ('slim', 'stout'), ('harsh', 'symphonic'), ('detract', 'sum'), ('useless', 'vital'), ('join', 'separation'), ('legal', 'unlawful'), ('temporary', 'terminal'), ('forgettable', 'iconic'), ('dismiss', 'ink'), ('distort', 'picture'), ('unnecessary', 'vital'), ('education', 'ignorance'), ('split', 'union'), ('medicine', 'poison'), ('desertion', 'stockade'), ('weighted', 'weightless'), ('certain', 'uninsured'), ('pastor', 'satanist'), ('distribution', 'single'), ('aridity', 'saturation'), ('tomato', 'township'), ('exact', 'vague'), ('abstract', 'photographic'), ('impartial', 'partial'), ('forever', 'wink'), ('spontaneous', 'taxonomic'), ('whisper', 'yell'), ('prohibition', 'saloon'), ('blandness', 'sensory'), ('flute', 'trombone'), ('mundane', 'mystical'), ('realism', 'symbolism'), ('destroy', 'fancy'), ('township', 'unincorporated'), ('refined', 'unrefined'), ('align', 'contrast'), ('parody', 'truth'), ('deep', 'short'), ('apparent', 'symbolic'), ('stationary', 'velocity'), ('bartering', 'fiscal'), ('halve', 'replicate'), ('clean', 'gross'), ('drama', 'parody'), ('literal', 'poetic'), ('herbivore', 'omnivore'), ('spirituality', 'taxonomic'), ('foot', 'head'), ('aid', 'skunk'), ('close', 'punch'), ('empty', 'matter'), ('average', 'iconic'), ('resonate', 'single'), ('nose', 'speed'), ('noticed', 'unnoticed'), ('faithful', 'skunk'), ('defend', 'implicate'), ('performing', 'training'), ('deaf', 'sensitive'), ('rigid', 'stretcher'), ('biblical', 'scientific'), ('clash', 'groove'), ('climatic', 'trivial'), ('musician', 'mute'), ('acceptable', 'unsatisfactory'), ('bowl', 'plate'), ('kiss', 'punch'), ('mastery', 'naivety'), ('futuristic', 'historic'), ('foreground', 'skyline'), ('none', 'skyline'), ('expensive', 'thrifty'), ('rural', 'township'), ('preferred', 'unfavorable'), ('pointless', 'symbolism'), ('profound', 'superficial'), ('snowball', 'water'), ('depress', 'pyramid'), ('structured', 'unstructured'), ('corroborate', 'invalidate'), ('single', 'tribal'), ('bore', 'intoxicate'), ('animals', 'nationality'), ('skyline', 'submarine'), ('fiery', 'oceanic'), ('expose', 'shield'), ('unemployment', 'workforce'), ('peaceful', 'sandstorm'), ('favorable', 'unfavorable'), ('pure', 'unrefined'), ('pat', 'rub'), ('clearness', 'sandstorm'), ('progressive', 'stable'), ('arbitrary', 'intentional'), ('poetic', 'rambling'), ('aggressor', 'caretaker'), ('achieve', 'fail'), ('inertia', 'velocity'), ('sand', 'stone'), ('ion', 'neutron'), ('taxonomic', 'unclassified'), ('pouch', 'uterine'), ('flawed', 'stone'), ('piano', 'trombone'), ('loaf', 'toil'), ('lumbar', 'pulmonary'), ('debonair', 'unrefined'), ('blindfold', 'compass'), ('parse', 'sum'), ('hold', 'spring'), ('builder', 'wrecker'), ('enliven', 'immobilize'), ('aggravation', 'alleviation'), ('astronomic', 'geologic'), ('follow', 'helm'), ('openness', 'stockade'), ('chin', 'foot'), ('salon', 'saloon'), ('extend', 'lop'), ('detoxify', 'intoxicate'), ('rendezvous', 'separate'), ('flashback', 'innovator'), ('diagnose', 'forget'), ('air', 'matter'), ('constant', 'orbital'), ('representative', 'unassociated'), ('curable', 'terminal'), ('alley', 'maze'), ('connect', 'divorce'), ('clown', 'educator'), ('stillness', 'stretcher'), ('famous', 'unknown'), ('ant', 'elephant'), ('fade', 'twinkle'), ('pyramid', 'weary'), ('disarray', 'preparedness'), ('avoidance', 'intercourse'), ('dispersal', 'rendezvous'), ('mental', 'pulmonary'), ('drain', 'power'), ('nonsense', 'rationale'), ('bureaucrat', 'musician'), ('darken', 'twinkle'), ('hearth', 'mantle'), ('commoner', 'emperor'), ('quarterly', 'yearly'), ('maintain', 'redirect'), ('abandon', 'hedge'), ('downfall', 'prime'), ('celibacy', 'intercourse'), ('loathe', 'pine'), ('depart', 'rendezvous'), ('mess', 'organise'), ('peach', 'tomato'), ('linear', 'orbital'), ('business', 'recreation'), ('critic', 'musician'), ('homage', 'parody'), ('ignorant', 'knowledgeable'), ('add', 'trim'), ('field', 'forest'), ('proportionate', 'unbalanced'), ('cynicism', 'naivety'), ('discordant', 'symphonic'), ('fluid', 'stone'), ('fictional', 'historic'), ('oceanic', 'shallow'), ('construct', 'deteriorate'), ('comply', 'helm'), ('dryness', 'saturation'), ('accept', 'critique'), ('increase', 'pine'), ('australian', 'kiwi'), ('human', 'zebra'), ('glance', 'tour'), ('acquittal', 'presentment'), ('symbolic', 'useless'), ('minor', 'prime'), ('delete', 'document'), ('blunt', 'hone'), ('monotone', 'poetic'), ('unimportant', 'vital'), ('frog', 'tsetse'), ('artichoke', 'kiwi'), ('alone', 'representative'), ('explained', 'pathological'), ('customize', 'replicate'), ('recognized', 'unnoticed'), ('fireball', 'snowball'), ('protect', 'punch'), ('destructive', 'instructive'), ('creator', 'wrecker'), ('abstraction', 'symbolism'), ('heed', 'wink'), ('layman', 'pastor'), ('phagocyte', 'virus'), ('clutter', 'organise'), ('bankrupt', 'funded'), ('fabric', 'knitwear'), ('be', 'picture'), ('gripper', 'slider'), ('inactivate', 'power'), ('economics', 'psychology'), ('criminal', 'pastor'), ('recoverable', 'unrecoverable'), ('flatten', 'pyramid'), ('seen', 'unnoticed'), ('cultured', 'unrefined'), ('antineutron', 'neutron'), ('iron', 'weaken'), ('cleansed', 'unclean'), ('confirmation', 'contradiction'), ('get', 'snowball'), ('ceo', 'workforce'), ('facilitate', 'redirect'), ('blend', 'contrast'), ('hesitation', 'rationale'), ('psychology', 'reality'), ('paucity', 'saturation'), ('dislike', 'pine'), ('lone', 'tribal'), ('even', 'uneven'), ('blurry', 'photographic'), ('left', 'right'), ('provident', 'wasteful'), ('subtract', 'tabulate'), ('adhere', 'lop'), ('space', 'zone'), ('backward', 'progressive'), ('relax', 'work'), ('unrelated', 'uterine'), ('cynical', 'unaffected'), ('lost', 'recoverable'), ('blot', 'ink'), ('symmetrical', 'uneven'), ('accountable', 'despotic'), ('freestyle', 'structure'), ('sit', 'spring'), ('atypical', 'representative'), ('ethereal', 'geologic'), ('excuse', 'rationale'), ('friendship', 'rivalry'), ('irrelevant', 'matter'), ('citizen', 'emperor'), ('heavy', 'lightweight'), ('configure', 'disjoint'), ('allow', 'edit'), ('restaurant', 'saloon'), ('stay', 'tour'), ('athlete', 'musician'), ('shirk', 'toil'), ('earthy', 'oceanic'), ('management', 'workforce'), ('arm', 'disarm'), ('breathless', 'pulmonary'), ('range', 'stockade'), ('stare', 'wink'), ('intentional', 'unintentional'), ('hone', 'lose'), ('puddle', 'snowball'), ('peasant', 'shah'), ('build', 'deteriorate'), ('freedom', 'stockade'), ('hate', 'like'), ('fold', 'whisk'), ('make', 'nature'), ('ladybug', 'zebra'), ('arrogant', 'humble'), ('bonus', 'fee'), ('solo', 'symphonic'), ('skinny', 'stout'), ('consolidation', 'distribution'), ('read', 'watch'), ('fire', 'ink'), ('learned', 'naivety'), ('okay', 'unsatisfactory'), ('defile', 'uplift'), ('regal', 'unrefined'), ('unemployed', 'workforce'), ('widen', 'wink'), ('broken', 'funded'), ('apple', 'orange'), ('opportunity', 'shield'), ('silence', 'talk'), ('insert', 'strum'), ('deceased', 'mortal'), ('thrifty', 'wasteful'), ('full', 'hunger'), ('bang', 'strum'), ('box', 'circle'), ('overlook', 'resonate'), ('bag', 'flask'), ('enlarge', 'trim'), ('flyer', 'slider'), ('incidental', 'indispensable'), ('straighten', 'zigzag'), ('destroyed', 'recoverable'), ('dispensable', 'indispensable'), ('probable', 'unlikely'), ('sandstorm', 'sunshine'), ('agnostic', 'pastor'), ('stagnation', 'velocity'), ('obviousness', 'symbolism'), ('boil', 'ice'), ('big', 'young'), ('recognize', 'unknown'), ('crowd', 'space'), ('sing', 'talk'), ('retire', 'work'), ('assemble', 'break'), ('attach', 'separate'), ('gentleman', 'player'), ('care', 'hate'), ('compact', 'spread'), ('sound', 'vision'), ('misunderstand', 'understand'), ('capital', 'small'), ('meet', 'pass'), ('modern', 'past'), ('appear', 'leave'), ('east', 'west'), ('blue', 'cloud'), ('melt', 'solid'), ('paper', 'rock'), ('press', 'pull'), ('fly', 'land'), ('breakfast', 'dinner'), ('black', 'light'), ('boy', 'female'), ('lip', 'nose'), ('road', 'street'), ('age', 'young'), ('decorate', 'plain'), ('mountain', 'plain'), ('happy', 'mean'), ('run', 'stop'), ('help', 'hurt'), ('house', 'tent'), ('have', 'miss'), ('life', 'mortal'), ('stiff', 'stretch'), ('brave', 'hide'), ('practice', 'test'), ('fear', 'safe'), ('aluminium', 'tin'), ('back', 'main'), ('hard', 'lazy'), ('negative', 'profit'), ('bright', 'smart'), ('column', 'row'), ('fail', 'success'), ('dirt', 'soap'), ('stone', 'water'), ('different', 'same'), ('homosexual', 'straight'), ('answer', 'pass'), ('ghost', 'real'), ('horizontal', 'vertical'), ('center', 'side'), ('pattern', 'plain'), ('race', 'slow'), ('use', 'waste'), ('loose', 'win'), ('fragile', 'strong'), ('animal', 'bug'), ('muscle', 'weak'), ('marriage', 'separation'), ('pressure', 'support'), ('busy', 'relax'), ('far', 'short'), ('pass', 'stop'), ('little', 'tall'), ('enter', 'exit'), ('bad', 'great'), ('cry', 'talk'), ('right', 'wrong'), ('keep', 'separation'), ('drink', 'solid'), ('low', 'tall'), ('communism', 'freedom'), ('stop', 'walk'), ('smart', 'thick'), ('recycle', 'waste'), ('healthy', 'poison'), ('save', 'waste'), ('male', 'woman'), ('home', 'work'), ('blow', 'create'), ('chill', 'heat'), ('heart', 'mind'), ('bottle', 'can'), ('hot', 'shade'), ('coin', 'note'), ('music', 'quiet'), ('gun', 'knife'), ('depression', 'happy'), ('dark', 'flash'), ('pay', 'steal'), ('nobody', 'person'), ('enemy', 'friendship'), ('float', 'sink'), ('copy', 'learn'), ('evil', 'nice'), ('enjoyment', 'pain'), ('down', 'raise'), ('move', 'stable'), ('edge', 'middle'), ('high', 'short'), ('collar', 'hem'), ('push', 'raise'), ('pepper', 'salt'), ('alphabet', 'count'), ('eye', 'hear'), ('adult', 'small'), ('defeat', 'success'), ('carpet', 'floor'), ('dress', 'strip'), ('good', 'poison'), ('door', 'hole'), ('silent', 'talk'), ('learn', 'stupid'), ('they', 'you'), ('floor', 'sky'), ('bounce', 'roll'), ('hide', 'view'), ('gentle', 'wild'), ('fat', 'health'), ('happen', 'peace'), ('earth', 'sea'), ('hour', 'second'), ('act', 'real'), ('real', 'test'), ('plan', 'surprise'), ('awake', 'rest'), ('lace', 'silk'), ('laugh', 'tear'), ('happiness', 'sadness'), ('past', 'present'), ('divine', 'nasty'), ('liquid', 'matter'), ('offspring', 'parent'), ('familiar', 'new'), ('gent', 'lady'), ('contract', 'grow'), ('burn', 'ice'), ('bone', 'flesh'), ('peace', 'violent'), ('garden', 'home'), ('history', 'news'), ('adventure', 'safe'), ('customer', 'worker'), ('cheap', 'value'), ('branch', 'join'), ('energy', 'lazy'), ('mistake', 'right'), ('slice', 'whole'), ('catch', 'free'), ('begin', 'middle'), ('credit', 'tax'), ('ceiling', 'wall'), ('courage', 'fear'), ('religion', 'science'), ('stale', 'sweet'), ('bold', 'shy'), ('action', 'rest'), ('north', 'west'), ('descend', 'rise'), ('fun', 'work'), ('donkey', 'horse'), ('log', 'stick'), ('strength', 'weak'), ('freeze', 'heat'), ('pleasure', 'unhappiness'), ('light', 'night'), ('irritate', 'mild'), ('metal', 'paper'), ('department', 'whole'), ('color', 'dull'), ('long', 'second'), ('ocean', 'sky'), ('bring', 'empty'), ('bolt', 'slow'), ('sick', 'well'), ('baby', 'old'), ('comfort', 'hard'), ('contain', 'out'), ('antique', 'new'), ('coward', 'noble'), ('night', 'sun'), ('drive', 'park'), ('bitch', 'dog'), ('dance', 'stand'), ('perfect', 'rubbish'), ('salt', 'sugar'), ('rush', 'slow'), ('leave', 'reach'), ('hill', 'plain'), ('farm', 'town'), ('juice', 'milk'), ('heterosexual', 'homosexual'), ('comfortable', 'fear'), ('balance', 'slip'), ('machine', 'person'), ('escape', 'prison'), ('cool', 'geek'), ('charge', 'payment'), ('beautiful', 'plain'), ('roll', 'stop'), ('harvest', 'plant'), ('lie', 'truth'), ('squash', 'stretch'), ('cup', 'plate'), ('bitter', 'sugar'), ('private', 'see'), ('fur', 'scale'), ('off', 'work'), ('refuse', 'take'), ('fast', 'slow'), ('reduce', 'rise'), ('look', 'touch'), ('late', 'present'), ('shade', 'sun'), ('excite', 'relax'), ('continent', 'sea'), ('rock', 'water'), ('glass', 'metal'), ('plant', 'rock'), ('corn', 'hay'), ('bed', 'seat'), ('innocent', 'trouble'), ('hall', 'room'), ('plain', 'unique'), ('chapter', 'page'), ('compound', 'simple'), ('lake', 'sea'), ('ascend', 'fall'), ('crime', 'justice'), ('active', 'tire'), ('benefit', 'punishment'), ('spot', 'stripe'), ('birthday', 'death'), ('nice', 'rank'), ('know', 'unknown'), ('cold', 'familiar'), ('everything', 'part'), ('song', 'talk'), ('butch', 'gentle'), ('play', 'rest'), ('honesty', 'lie'), ('hollow', 'solid'), ('body', 'leg'), ('middle', 'side'), ('grass', 'water'), ('bumpy', 'level'), ('anger', 'happy'), ('rapid', 'slow'), ('taste', 'vision'), ('professor', 'student'), ('injure', 'praise'), ('sky', 'water'), ('letter', 'number'), ('meat', 'plant'), ('cover', 'discover'), ('joy', 'pain'), ('nothing', 'sound'), ('lift', 'press'), ('aisle', 'seat'), ('hear', 'see'), ('can', 'jar'), ('character', 'dull'), ('industry', 'nature'), ('milk', 'water'), ('blade', 'handle'), ('accident', 'purpose'), ('base', 'head'), ('design', 'destroy'), ('student', 'teacher'), ('mobile', 'stationary'), ('hobby', 'work'), ('committee', 'individual'), ('joint', 'separate'), ('discover', 'loose'), ('closet', 'out'), ('fool', 'smart'), ('exercise', 'rest'), ('blood', 'bone'), ('boss', 'worker'), ('bush', 'tree'), ('lot', 'small'), ('snow', 'sun'), ('damage', 'protect'), ('water', 'wood'), ('advance', 'late'), ('bath', 'shower'), ('ear', 'mouth'), ('game', 'work'), ('sour', 'sugar'), ('number', 'word'), ('sharp', 'thick'), ('die', 'exist'), ('future', 'history'), ('captain', 'follower'), ('erect', 'soft'), ('touch', 'vision'), ('slip', 'stand'), ('extinguish', 'light'), ('growth', 'reduce'), ('fork', 'knife'), ('everyone', 'nobody'), ('jog', 'walk'), ('all', 'piece'), ('wall', 'window'), ('same', 'unique'), ('affection', 'dislike'), ('call', 'text'), ('paint', 'paper'), ('heat', 'shade'), ('gay', 'straight'), ('bedroom', 'kitchen'), ('seat', 'stand'), ('ordinary', 'weird'), ('belief', 'fact'), ('strong', 'vulnerable'), ('useful', 'waste'), ('female', 'son'), ('divorce', 'engage'), ('foolish', 'smart'), ('company', 'loneliness'), ('ice', 'steam'), ('leg', 'toe'), ('mineral', 'vegetable'), ('dwell', 'move'), ('concentrate', 'distribute'), ('lady', 'male'), ('brake', 'pedal'), ('might', 'weak'), ('girl', 'he'), ('alive', 'disease'), ('disease', 'well'), ('funeral', 'party'), ('ferment', 'sweet'), ('mystery', 'open'), ('it', 'we'), ('pale', 'tan'), ('show', 'watch'), ('man', 'wife'), ('rise', 'sunset'), ('mild', 'violent'), ('head', 'rear'), ('common', 'royal'), ('marine', 'navy'), ('new', 'use'), ('miss', 'see'), ('day', 'night'), ('disgust', 'nice'), ('city', 'nature'), ('gold', 'rubbish'), ('steal', 'trade'), ('rough', 'silk'), ('lazy', 'work'), ('important', 'useless'), ('soap', 'water'), ('finger', 'foot'), ('due', 'pay'), ('pain', 'relief'), ('seed', 'tree'), ('mean', 'nice'), ('bullet', 'safe'), ('food', 'poison'), ('large', 'little'), ('conflict', 'peace'), ('art', 'ugly'), ('relaxation', 'work'), ('charm', 'dull'), ('gas', 'solid'), ('copper', 'iron'), ('test', 'work'), ('heel', 'toe'), ('cd', 'record'), ('complex', 'plain'), ('party', 'study'), ('stitch', 'tear'), ('animate', 'live'), ('bow', 'pluck'), ('positive', 'uncertain'), ('outside', 'room'), ('potato', 'rice'), ('quality', 'rubbish'), ('morning', 'night'), ('minute', 'second'), ('one', 'zero'), ('poison', 'safe'), ('leak', 'seal'), ('safe', 'trouble'), ('he', 'woman'), ('detail', 'general'), ('performance', 'practice'), ('club', 'heart'), ('expand', 'reduce'), ('come', 'leave'), ('false', 'science'), ('chain', 'free'), ('oil', 'water'), ('steel', 'weak'), ('climb', 'fall'), ('crop', 'expand'), ('sheep', 'wolf'), ('benevolent', 'evil'), ('naughty', 'nice'), ('group', 'person'), ('straight', 'turn'), ('kind', 'mean'), ('control', 'free'), ('exist', 'nothing'), ('dash', 'dot'), ('enjoy', 'hate'), ('record', 'tape'), ('intelligent', 'stupid'), ('bleach', 'stain'), ('huge', 'small'), ('draw', 'trace'), ('pull', 'reach'), ('standard', 'strange'), ('want', 'waste'), ('gate', 'wall'), ('army', 'marine'), ('powerful', 'vulnerable'), ('intense', 'mild'), ('lyric', 'tune'), ('law', 'theory'), ('school', 'work'), ('screen', 'stage'), ('old', 'young'), ('solid', 'soup'), ('magic', 'science'), ('line', 'point'), ('crook', 'level'), ('delight', 'unhappy'), ('bull', 'chicken'), ('office', 'outside'), ('rule', 'subject'), ('idea', 'material'), ('dream', 'real'), ('money', 'payment'), ('loud', 'soft'), ('family', 'friend'), ('interior', 'outside'), ('separation', 'union'), ('cloudy', 'sunshine'), ('cement', 'mortar'), ('choose', 'refuse'), ('change', 'stable'), ('electron', 'neutron'), ('curve', 'square'), ('bark', 'bite'), ('punishment', 'reward'), ('movie', 'play'), ('arithmetic', 'read'), ('crater', 'hill'), ('stand', 'walk'), ('lightning', 'thunder'), ('energetic', 'lazy'), ('class', 'recess'), ('plate', 'table'), ('flow', 'stick'), ('bat', 'field'), ('talk', 'write'), ('simple', 'smart'), ('root', 'trunk'), ('pupil', 'teacher'), ('thick', 'thin'), ('darkness', 'sunshine'), ('labor', 'machine'), ('sign', 'talk'), ('story', 'truth'), ('reward', 'stick'), ('book', 'film'), ('parent', 'son'), ('bread', 'meat'), ('short', 'tall'), ('beef', 'chicken'), ('elevator', 'stair'), ('cell', 'freedom'), ('satisfaction', 'unhappy'), ('hurt', 'protect'), ('cavity', 'fill'), ('vacation', 'work'), ('circle', 'line'), ('grind', 'play'), ('hatred', 'love'), ('exchange', 'keep'), ('beach', 'sea'), ('stretch', 'tight'), ('noise', 'silent'), ('store', 'trade'), ('bloom', 'die'), ('gum', 'tooth'), ('beauty', 'ugly'), ('garbage', 'valuable'), ('movement', 'quiet'), ('valuable', 'waste'), ('pan', 'pot'), ('enlightenment', 'ignorance'), ('confirm', 'question'), ('cat', 'mouse'), ('generous', 'mean'), ('both', 'one'), ('fine', 'pain'), ('great', 'less'), ('hardware', 'software'), ('beer', 'wine'), ('attic', 'basement'), ('forest', 'plain'), ('coffee', 'tea'), ('confidence', 'fear'), ('everybody', 'nobody'), ('object', 'person'), ('confusion', 'peace'), ('alcohol', 'water'), ('health', 'sick'), ('duck', 'stand'), ('shoe', 'sock'), ('handle', 'spout'), ('shadow', 'sun'), ('kitten', 'puppy'), ('sidewalk', 'street'), ('ride', 'walk'), ('decide', 'think'), ('father', 'offspring'), ('affair', 'spouse'), ('problem', 'solution'), ('fortune', 'trouble'), ('sickness', 'well'), ('dog', 'kitten'), ('me', 'you'), ('men', 'woman'), ('lemon', 'orange'), ('hail', 'rain'), ('nasty', 'respect'), ('sight', 'taste'), ('trip', 'vacation'), ('pretend', 'real'), ('river', 'sea'), ('see', 'touch'), ('mature', 'young'), ('speak', 'write'), ('needle', 'thread'), ('bear', 'lion'), ('admire', 'dislike'), ('bind', 'loose'), ('boyfriend', 'girl'), ('spit', 'swallow'), ('friend', 'strange'), ('drown', 'swim'), ('hit', 'miss'), ('factory', 'farm'), ('cigarette', 'pipe'), ('bike', 'car'), ('flight', 'walk'), ('hostility', 'peace'), ('imagination', 'real'), ('fraud', 'true'), ('fix', 'mobile'), ('sleep', 'wake'), ('green', 'red'), ('tell', 'watch'), ('love', 'war'), ('angel', 'evil'), ('radio', 'tv'), ('bathroom', 'bedroom'), ('sister', 'son'), ('friendly', 'mean'), ('we', 'you'), ('sea', 'sky'), ('hop', 'jump'), ('armor', 'clothe'), ('cheese', 'milk'), ('defense', 'prosecution'), ('general', 'unique'), ('kindness', 'mean'), ('original', 'reproduction'), ('death', 'liberty'), ('error', 'perfect'), ('royal', 'subject'), ('brass', 'silver'), ('child', 'old'), ('cotton', 'wool'), ('doctor', 'patient'), ('god', 'mortal'), ('china', 'japan'), ('fasten', 'loose'), ('material', 'spirit')]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = defaultdict(list)\n",
    "print(similarity_matrix)\n",
    "for each_pair in tqdm(list_antonym):\n",
    "    word1 = each_pair[0]\n",
    "    word2 = each_pair[1]\n",
    "    #print(each_pair)\n",
    "    #print(word1 < word2)\n",
    "    \n",
    "    if word1 < word2: ## welches wort l√§nger ist\n",
    "        similarity_matrix[word1].append(word2)                  #defaultdict(<class 'list'>, {'exclude': ['inscribe']})\n",
    "        #print(similarity_matrix)\n",
    "    else:\n",
    "        similarity_matrix[word2].append(word1)\n",
    "    \n",
    "all_similarity = defaultdict(dict)\n",
    "for each_key in tqdm(similarity_matrix):\n",
    "    for each_value in similarity_matrix[each_key]:\n",
    "#         cosine_similarity([current_model[each_key]]\n",
    "        all_similarity[each_key][each_value] = abs(cosine_similarity([current_model.wv[each_key]],[current_model.wv[each_value]])[0][0])\n",
    "        #print(current_model[each_key])\n",
    "        #print(current_model[each_value])\n",
    "        #print(all_similarity[each_key][each_value])\n",
    "       \n",
    "final_antonym_list = []\n",
    "for index_counter, each_key in enumerate(tqdm(all_similarity)):\n",
    "#     print(each_key,all_similarity[each_key])\n",
    "    listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "    #print(listofTuples)\n",
    "    final_antonym_list.append((each_key, listofTuples[0][0]))\n",
    " \n",
    "print(final_antonym_list)\n",
    "\n",
    "list_antonym = final_antonym_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1409\n"
     ]
    }
   ],
   "source": [
    "print(len(list_antonym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_antonym = 50#1468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1409, 300)\n"
     ]
    }
   ],
   "source": [
    "## Find the antonym difference vectors\n",
    "antonymy_vector = []\n",
    "for each_word_pair in list_antonym:\n",
    "    antonymy_vector.append(current_model.wv[each_word_pair[0]]- current_model.wv[each_word_pair[1]])\n",
    "antonymy_vector = np.array(antonymy_vector)\n",
    "print(antonymy_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('beginning', 'terminal')\n"
     ]
    }
   ],
   "source": [
    "print(list_antonym[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Dimension Selection Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b2e9a80d5146ffaf91166b157748dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1409), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from scipy.spatial.distance import cosine as scipy_cosine\n",
    "random.seed(42)\n",
    "\n",
    "t1 = np.array(antonymy_vector)\n",
    "dimension_similarity_matrix = defaultdict(dict)\n",
    "for index_1, each_dim1 in enumerate(tqdm(t1)):\n",
    "    for index_2, each_dim2 in enumerate(t1):\n",
    "        dimension_similarity_matrix[index_1][index_2] = abs(1-scipy_cosine(each_dim1, each_dim2))\n",
    "        \n",
    "        \n",
    "def get_set_score(final_list, each_dim):\n",
    "    final_output = 0.0\n",
    "    for each_vec in final_list:\n",
    "        final_output += dimension_similarity_matrix[each_vec][each_dim]\n",
    "    return final_output/(len(final_list))\n",
    "        \n",
    "def select_subset_dimension(dim_vector, num_dim):\n",
    "    working_list = np.array(dim_vector)\n",
    "    \n",
    "    working_position_index = [i for i in range(working_list.shape[0])]\n",
    "    final_position_index = []\n",
    "    \n",
    "\n",
    "    print('working list is ready, shape', working_list.shape)\n",
    "    sel_dim = random.randrange(0, working_list.shape[0])\n",
    "\n",
    "    final_position_index.append(sel_dim)\n",
    "    \n",
    "    working_position_index.remove(sel_dim)\n",
    "\n",
    "    \n",
    "    for test_count in tqdm(range(num_dim-1)):\n",
    "        min_dim = None\n",
    "        min_score = 1000\n",
    "        for temp_index, each_dim in enumerate(working_position_index):\n",
    "#             print(each_dim)\n",
    "            temp_score = get_set_score(final_position_index, each_dim)\n",
    "            if temp_score< min_score:\n",
    "                min_score= temp_score\n",
    "                min_dim = each_dim\n",
    "        print(test_count,min_dim)\n",
    "        final_position_index.append(min_dim)\n",
    "        working_position_index.remove(min_dim)\n",
    "#         print(working_list.shape, len(final_list))\n",
    "    return final_position_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the ORTHOGONAL DIMENSION Order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the ORTHOGONAL DIMENSION Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working list is ready, shape (1409, 300)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34598d3fe34f4ce0b1a63e104420ff07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=499), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 745\n",
      "1 828\n",
      "2 824\n",
      "3 32\n",
      "4 853\n",
      "5 1040\n",
      "6 537\n",
      "7 799\n",
      "8 1275\n",
      "9 789\n",
      "10 954\n",
      "11 1165\n",
      "12 284\n",
      "13 1057\n",
      "14 217\n",
      "15 1323\n",
      "16 939\n",
      "17 1363\n",
      "18 714\n",
      "19 830\n",
      "20 780\n",
      "21 492\n",
      "22 385\n",
      "23 1031\n",
      "24 874\n",
      "25 205\n",
      "26 1000\n",
      "27 1012\n",
      "28 281\n",
      "29 1318\n",
      "30 1393\n",
      "31 125\n",
      "32 1136\n",
      "33 784\n",
      "34 1260\n",
      "35 722\n",
      "36 766\n",
      "37 1187\n",
      "38 982\n",
      "39 1049\n",
      "40 876\n",
      "41 230\n",
      "42 123\n",
      "43 870\n",
      "44 1266\n",
      "45 1103\n",
      "46 330\n",
      "47 1332\n",
      "48 975\n",
      "49 995\n",
      "50 1074\n",
      "51 723\n",
      "52 429\n",
      "53 567\n",
      "54 1252\n",
      "55 1099\n",
      "56 928\n",
      "57 1194\n",
      "58 552\n",
      "59 150\n",
      "60 1219\n",
      "61 1090\n",
      "62 1369\n",
      "63 1128\n",
      "64 1117\n",
      "65 176\n",
      "66 815\n",
      "67 1351\n",
      "68 924\n",
      "69 1095\n",
      "70 1381\n",
      "71 251\n",
      "72 1347\n",
      "73 768\n",
      "74 936\n",
      "75 1097\n",
      "76 317\n",
      "77 1179\n",
      "78 786\n",
      "79 1325\n",
      "80 1388\n",
      "81 1256\n",
      "82 1372\n",
      "83 104\n",
      "84 889\n",
      "85 1320\n",
      "86 397\n",
      "87 94\n",
      "88 1043\n",
      "89 1246\n",
      "90 491\n",
      "91 1085\n",
      "92 243\n",
      "93 988\n",
      "94 1064\n",
      "95 535\n",
      "96 1105\n",
      "97 1156\n",
      "98 1343\n",
      "99 1160\n",
      "100 412\n",
      "101 126\n",
      "102 1247\n",
      "103 1037\n",
      "104 769\n",
      "105 1279\n",
      "106 1201\n",
      "107 244\n",
      "108 838\n",
      "109 1168\n",
      "110 1354\n",
      "111 1149\n",
      "112 1015\n",
      "113 1401\n",
      "114 985\n",
      "115 302\n",
      "116 1382\n",
      "117 1342\n",
      "118 321\n",
      "119 641\n",
      "120 1046\n",
      "121 1206\n",
      "122 1200\n",
      "123 476\n",
      "124 885\n",
      "125 1330\n",
      "126 1387\n",
      "127 1129\n",
      "128 1203\n",
      "129 1051\n",
      "130 1293\n",
      "131 506\n",
      "132 1406\n",
      "133 1262\n",
      "134 1277\n",
      "135 750\n",
      "136 1217\n",
      "137 1052\n",
      "138 367\n",
      "139 1034\n",
      "140 1008\n",
      "141 548\n",
      "142 331\n",
      "143 231\n",
      "144 908\n",
      "145 85\n",
      "146 27\n",
      "147 862\n",
      "148 483\n",
      "149 460\n",
      "150 1214\n",
      "151 1295\n",
      "152 1236\n",
      "153 556\n",
      "154 1257\n",
      "155 1328\n",
      "156 1182\n",
      "157 1344\n",
      "158 1068\n",
      "159 1287\n",
      "160 1249\n",
      "161 595\n",
      "162 1081\n",
      "163 70\n",
      "164 1044\n",
      "165 834\n",
      "166 1364\n",
      "167 850\n",
      "168 339\n",
      "169 1232\n",
      "170 1111\n",
      "171 806\n",
      "172 744\n",
      "173 1258\n",
      "174 1270\n",
      "175 948\n",
      "176 1269\n",
      "177 913\n",
      "178 529\n",
      "179 888\n",
      "180 909\n",
      "181 418\n",
      "182 1153\n",
      "183 1227\n",
      "184 661\n",
      "185 34\n",
      "186 1114\n",
      "187 546\n",
      "188 289\n",
      "189 25\n",
      "190 1384\n",
      "191 861\n",
      "192 1147\n",
      "193 1362\n",
      "194 35\n",
      "195 917\n",
      "196 1180\n",
      "197 683\n",
      "198 971\n",
      "199 907\n",
      "200 1132\n",
      "201 946\n",
      "202 571\n",
      "203 875\n",
      "204 1036\n",
      "205 1155\n",
      "206 854\n",
      "207 1359\n",
      "208 1271\n",
      "209 1241\n",
      "210 781\n",
      "211 671\n",
      "212 334\n",
      "213 1385\n",
      "214 598\n",
      "215 795\n",
      "216 1374\n",
      "217 380\n",
      "218 1192\n",
      "219 1106\n",
      "220 1376\n",
      "221 1014\n",
      "222 845\n",
      "223 947\n",
      "224 965\n",
      "225 66\n",
      "226 1352\n",
      "227 344\n",
      "228 1265\n",
      "229 293\n",
      "230 188\n",
      "231 1245\n",
      "232 1231\n",
      "233 1333\n",
      "234 42\n",
      "235 458\n",
      "236 564\n",
      "237 554\n",
      "238 1152\n",
      "239 493\n",
      "240 837\n",
      "241 484\n",
      "242 522\n",
      "243 1305\n",
      "244 716\n",
      "245 1228\n",
      "246 1161\n",
      "247 764\n",
      "248 182\n",
      "249 1321\n",
      "250 1042\n",
      "251 233\n",
      "252 438\n",
      "253 141\n",
      "254 93\n",
      "255 378\n",
      "256 324\n",
      "257 1394\n",
      "258 1159\n",
      "259 470\n",
      "260 1263\n",
      "261 1327\n",
      "262 902\n",
      "263 1190\n",
      "264 863\n",
      "265 921\n",
      "266 731\n",
      "267 313\n",
      "268 1131\n",
      "269 1146\n",
      "270 847\n",
      "271 1307\n",
      "272 1264\n",
      "273 891\n",
      "274 1218\n",
      "275 51\n",
      "276 222\n",
      "277 501\n",
      "278 1223\n",
      "279 672\n",
      "280 616\n",
      "281 1350\n",
      "282 1391\n",
      "283 1170\n",
      "284 279\n",
      "285 405\n",
      "286 1216\n",
      "287 478\n",
      "288 712\n",
      "289 673\n",
      "290 1101\n",
      "291 1326\n",
      "292 911\n",
      "293 148\n",
      "294 1030\n",
      "295 5\n",
      "296 373\n",
      "297 1319\n",
      "298 147\n",
      "299 1297\n",
      "300 1268\n",
      "301 1077\n",
      "302 943\n",
      "303 835\n",
      "304 844\n",
      "305 338\n",
      "306 1317\n",
      "307 347\n",
      "308 261\n",
      "309 1079\n",
      "310 1091\n",
      "311 649\n",
      "312 1331\n",
      "313 690\n",
      "314 942\n",
      "315 886\n",
      "316 627\n",
      "317 577\n",
      "318 1229\n",
      "319 821\n",
      "320 1244\n",
      "321 473\n",
      "322 963\n",
      "323 1186\n",
      "324 559\n",
      "325 266\n",
      "326 1276\n",
      "327 1016\n",
      "328 215\n",
      "329 734\n",
      "330 620\n",
      "331 422\n",
      "332 1340\n",
      "333 782\n",
      "334 1357\n",
      "335 1066\n",
      "336 345\n",
      "337 592\n",
      "338 740\n",
      "339 441\n",
      "340 165\n",
      "341 1145\n",
      "342 945\n",
      "343 866\n",
      "344 538\n",
      "345 1367\n",
      "346 1243\n",
      "347 949\n",
      "348 1082\n",
      "349 1312\n",
      "350 1166\n",
      "351 1024\n",
      "352 430\n",
      "353 785\n",
      "354 139\n",
      "355 603\n",
      "356 926\n",
      "357 1125\n",
      "358 465\n",
      "359 466\n",
      "360 350\n",
      "361 1395\n",
      "362 489\n",
      "363 933\n",
      "364 490\n",
      "365 935\n",
      "366 842\n",
      "367 1010\n",
      "368 951\n",
      "369 210\n",
      "370 561\n",
      "371 808\n",
      "372 1028\n",
      "373 1019\n",
      "374 1199\n",
      "375 974\n",
      "376 1033\n",
      "377 1002\n",
      "378 848\n",
      "379 791\n",
      "380 1225\n",
      "381 399\n",
      "382 329\n",
      "383 678\n",
      "384 802\n",
      "385 903\n",
      "386 884\n",
      "387 760\n",
      "388 1290\n",
      "389 648\n",
      "390 273\n",
      "391 959\n",
      "392 859\n",
      "393 1115\n",
      "394 759\n",
      "395 519\n",
      "396 1273\n",
      "397 1127\n",
      "398 1251\n",
      "399 1130\n",
      "400 1056\n",
      "401 218\n",
      "402 1069\n",
      "403 817\n",
      "404 1242\n",
      "405 1215\n",
      "406 1093\n",
      "407 957\n",
      "408 1104\n",
      "409 1371\n",
      "410 864\n",
      "411 520\n",
      "412 581\n",
      "413 1248\n",
      "414 560\n",
      "415 97\n",
      "416 605\n",
      "417 981\n",
      "418 411\n",
      "419 386\n",
      "420 1113\n",
      "421 410\n",
      "422 26\n",
      "423 203\n",
      "424 214\n",
      "425 725\n",
      "426 826\n",
      "427 305\n",
      "428 1313\n",
      "429 237\n",
      "430 263\n",
      "431 993\n",
      "432 1348\n",
      "433 167\n",
      "434 1109\n",
      "435 232\n",
      "436 1195\n",
      "437 1141\n",
      "438 668\n",
      "439 477\n",
      "440 856\n",
      "441 1197\n",
      "442 1163\n",
      "443 1283\n",
      "444 1018\n",
      "445 843\n",
      "446 128\n",
      "447 494\n",
      "448 694\n",
      "449 221\n",
      "450 8\n",
      "451 475\n",
      "452 1379\n",
      "453 1373\n",
      "454 1071\n",
      "455 1110\n",
      "456 60\n",
      "457 1404\n",
      "458 846\n",
      "459 777\n",
      "460 1296\n",
      "461 298\n",
      "462 1207\n",
      "463 101\n",
      "464 1280\n",
      "465 613\n",
      "466 931\n",
      "467 1120\n",
      "468 702\n",
      "469 1386\n",
      "470 312\n",
      "471 765\n",
      "472 1022\n",
      "473 142\n",
      "474 239\n",
      "475 1003\n",
      "476 852\n",
      "477 1402\n",
      "478 140\n",
      "479 256\n",
      "480 40\n",
      "481 1310\n",
      "482 1205\n",
      "483 282\n",
      "484 83\n",
      "485 84\n",
      "486 839\n",
      "487 1272\n",
      "488 1102\n",
      "489 306\n",
      "490 216\n",
      "491 1403\n",
      "492 299\n",
      "493 708\n",
      "494 502\n",
      "495 868\n",
      "496 272\n",
      "497 434\n",
      "498 1337\n",
      "\n",
      "(500,)\n",
      "[ 617  745  828  824   32  853 1040  537  799 1275  789  954 1165  284\n",
      " 1057  217 1323  939 1363  714  830  780  492  385 1031  874  205 1000\n",
      " 1012  281 1318 1393  125 1136  784 1260  722  766 1187  982 1049  876\n",
      "  230  123  870 1266 1103  330 1332  975  995 1074  723  429  567 1252\n",
      " 1099  928 1194  552  150 1219 1090 1369 1128 1117  176  815 1351  924\n",
      " 1095 1381  251 1347  768  936 1097  317 1179  786 1325 1388 1256 1372\n",
      "  104  889 1320  397   94 1043 1246  491 1085  243  988 1064  535 1105\n",
      " 1156 1343 1160  412  126 1247 1037  769 1279 1201  244  838 1168 1354\n",
      " 1149 1015 1401  985  302 1382 1342  321  641 1046 1206 1200  476  885\n",
      " 1330 1387 1129 1203 1051 1293  506 1406 1262 1277  750 1217 1052  367\n",
      " 1034 1008  548  331  231  908   85   27  862  483  460 1214 1295 1236\n",
      "  556 1257 1328 1182 1344 1068 1287 1249  595 1081   70 1044  834 1364\n",
      "  850  339 1232 1111  806  744 1258 1270  948 1269  913  529  888  909\n",
      "  418 1153 1227  661   34 1114  546  289   25 1384  861 1147 1362   35\n",
      "  917 1180  683  971  907 1132  946  571  875 1036 1155  854 1359 1271\n",
      " 1241  781  671  334 1385  598  795 1374  380 1192 1106 1376 1014  845\n",
      "  947  965   66 1352  344 1265  293  188 1245 1231 1333   42  458  564\n",
      "  554 1152  493  837  484  522 1305  716 1228 1161  764  182 1321 1042\n",
      "  233  438  141   93  378  324 1394 1159  470 1263 1327  902 1190  863\n",
      "  921  731  313 1131 1146  847 1307 1264  891 1218   51  222  501 1223\n",
      "  672  616 1350 1391 1170  279  405 1216  478  712  673 1101 1326  911\n",
      "  148 1030    5  373 1319  147 1297 1268 1077  943  835  844  338 1317\n",
      "  347  261 1079 1091  649 1331  690  942  886  627  577 1229  821 1244\n",
      "  473  963 1186  559  266 1276 1016  215  734  620  422 1340  782 1357\n",
      " 1066  345  592  740  441  165 1145  945  866  538 1367 1243  949 1082\n",
      " 1312 1166 1024  430  785  139  603  926 1125  465  466  350 1395  489\n",
      "  933  490  935  842 1010  951  210  561  808 1028 1019 1199  974 1033\n",
      " 1002  848  791 1225  399  329  678  802  903  884  760 1290  648  273\n",
      "  959  859 1115  759  519 1273 1127 1251 1130 1056  218 1069  817 1242\n",
      " 1215 1093  957 1104 1371  864  520  581 1248  560   97  605  981  411\n",
      "  386 1113  410   26  203  214  725  826  305 1313  237  263  993 1348\n",
      "  167 1109  232 1195 1141  668  477  856 1197 1163 1283 1018  843  128\n",
      "  494  694  221    8  475 1379 1373 1071 1110   60 1404  846  777 1296\n",
      "  298 1207  101 1280  613  931 1120  702 1386  312  765 1022  142  239\n",
      " 1003  852 1402  140  256   40 1310 1205  282   83   84  839 1272 1102\n",
      "  306  216 1403  299  708  502  868  272  434 1337]\n"
     ]
    }
   ],
   "source": [
    "num_antonym = 500\n",
    "orthogonal_antonymy_vector =np.array(select_subset_dimension(antonymy_vector, num_antonym))  \n",
    "print(orthogonal_antonymy_vector.shape)\n",
    "print(orthogonal_antonymy_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the RANDOM DIMENSION Order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the RANDOM DIMENSION Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1409\n"
     ]
    }
   ],
   "source": [
    "random_antonym_vector = [i for i in range(len(antonymy_vector))]\n",
    "random.shuffle(random_antonym_vector)\n",
    "print(len(random_antonym_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the MAXIMUM VARIANCE DIMENSION Order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the MAXIMUM VARIANCE DIMENSION Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding size is 1409\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633327f616674bb4a85d51a55d8512c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=147612), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_size = antonymy_vector.shape[0]   \n",
    "print('The embedding size is', embedding_size)\n",
    "\n",
    "\n",
    "variance_antonymy_vector_inverse = np.linalg.pinv(np.transpose(antonymy_vector))\n",
    "\n",
    "embedding_matrix = []\n",
    "\n",
    "\n",
    "total_words = 0\n",
    "for each_word in tqdm(current_model.wv.vocab):\n",
    "    total_words += 1\n",
    "\n",
    "    new_vector = np.matmul(variance_antonymy_vector_inverse,current_model[each_word])\n",
    "    \n",
    "    embedding_matrix.append(new_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5665c13eda7d49c6993a8aa592e8459e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1409), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "variance_list = []\n",
    "\n",
    "embedding_matrix = np.array(embedding_matrix)\n",
    "\n",
    "for each_dimension in tqdm(range(embedding_matrix.shape[1])):\n",
    "    variance_list.append(np.var(embedding_matrix[:,each_dimension]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_antonymy_vector = [each for each in sorted(range(len(variance_list)), key=lambda i: variance_list[i], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "del variance_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation to polar space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation to polar space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_antonym_space(current_model, output_file_path, binary, current_antonymy_vector_inverse):\n",
    "    embedding_size = orthogonal_antonymy_vector.shape[0] #current_antonymy_vector_inverse.shape[0]   ##CHANGE THIS ACCORDINGLY!!!\n",
    "    print('New model size is',len(current_model.wv.vocab), embedding_size)\n",
    "\n",
    "    temp_file = None\n",
    "    \n",
    "    if binary:\n",
    "        temp_file = open(output_file_path,'wb')\n",
    "        temp_file.write(str.encode(str(len(current_model.wv.vocab))+' '+str(embedding_size)+'\\n'))\n",
    "    else:\n",
    "        temp_file = open(output_file_path,'w')\n",
    "        temp_file.write(str(len(current_model.wv.vocab))+' '+str(embedding_size)+'\\n')\n",
    "\n",
    "    total_words = 0\n",
    "    for each_word in tqdm(current_model.wv.vocab):\n",
    "        total_words += 1\n",
    "        if binary:\n",
    "            temp_file.write(str.encode(each_word+' '))\n",
    "        else:\n",
    "            temp_file.write(each_word+' ')\n",
    "\n",
    "        new_vector = np.matmul(current_antonymy_vector_inverse,current_model[each_word])\n",
    "\n",
    "        new_vector = new_vector/linalg.norm(new_vector)\n",
    "\n",
    "        \n",
    "        \n",
    "        if binary:\n",
    "            temp_file.write(new_vector)\n",
    "            temp_file.write(str.encode('\\n'))\n",
    "        else:\n",
    "            temp_file.write(str(new_vector))\n",
    "            temp_file.write('\\n')\n",
    "\n",
    "\n",
    "    temp_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard normal transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_normal_dist_model(model, new_filename):\n",
    "    embedding_matrix = []\n",
    "    embedding_vocab = []\n",
    "\n",
    "    temp_file = open(new_filename,'wb')\n",
    "    temp_file.write(str.encode(str(model.vectors.shape[0])+' '+str(model.vectors.shape[1])+'\\n'))\n",
    "    \n",
    "    for each_word in tqdm(model.wv.vocab):\n",
    "        embedding_matrix.append(model[each_word])\n",
    "        embedding_vocab.append(each_word)\n",
    "    \n",
    "    embedding_matrix = np.array(embedding_matrix)\n",
    "    \n",
    "    print('The shape of embedding matrix is {}'.format(embedding_matrix.shape))\n",
    "    \n",
    "    norm_embedding_matrix = (embedding_matrix - embedding_matrix.mean(0))/ embedding_matrix.std(0)\n",
    "    \n",
    "    for word_counter, each_word in enumerate(tqdm(embedding_vocab)):\n",
    "#         assert each_word==embedding_vocab[word_counter],'Not matching!!!'\n",
    "        \n",
    "        temp_file.write(str.encode(each_word+' '))\n",
    "        new_vector = norm_embedding_matrix[word_counter]\n",
    "        temp_file.write(new_vector)\n",
    "        temp_file.write(str.encode('\\n'))\n",
    "        \n",
    "    del embedding_matrix\n",
    "    del embedding_vocab\n",
    "    temp_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the task score for different dimension size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding_path(current_model, embedding_path, binary, antonym_vector, curr_dim):\n",
    "    curr_antonym_vector = antonymy_vector[antonym_vector[:curr_dim]]\n",
    "    curr_antonymy_vector_inverse = np.linalg.pinv(np.transpose(curr_antonym_vector))\n",
    "    transform_to_antonym_space(current_model, embedding_path, binary,curr_antonymy_vector_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "#dim_size = 500 # Number of POLAR dimenions\n",
    "antonym_vector_method = orthogonal_antonymy_vector#random_antonym_vector # orthogonal_antonymy_vector, variance_antonymy_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model size is 147612 500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2e456ecb664167b8306433bf9b61aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=147612), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading the model\n",
      "loading done..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd114d47a1124b1b85faaad800abcd08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=147612), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The shape of embedding matrix is (147612, 500)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093b0fd757d748468e8101d94d16e2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=147612), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "curr_dim = len(antonym_vector_method)#dim_size\n",
    "\n",
    "embedding_path = r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\Semantic-Differentials-for-Wikipedia-using-the-POLAR-Framework\\JanEngler\\Praktikum\\polarEmbedding'+str(curr_dim)+'.bin'\n",
    "generate_embedding_path(current_model, embedding_path,True,antonym_vector_method, curr_dim)\n",
    "\n",
    "print('loading the model')\n",
    "temp_model = gensim.models.KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n",
    "print('loading done..')\n",
    "\n",
    "std_nrml_embedding_path = r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\Semantic-Differentials-for-Wikipedia-using-the-POLAR-Framework\\JanEngler\\Praktikum\\polarEmbedding'+str(curr_dim)+'_StdNrml.bin'\n",
    "standard_normal_dist_model(temp_model, std_nrml_embedding_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Loading\n"
     ]
    }
   ],
   "source": [
    "#curr_dim=500\n",
    "path=r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\Semantic-Differentials-for-Wikipedia-using-the-POLAR-Framework\\JanEngler\\Praktikum\\polarEmbedding'+str(curr_dim)+'_StdNrml.bin'\n",
    "#f = open(path, \"r+b\")\n",
    "#f.seek(0)\n",
    "#print(f)\n",
    "word2vecPolar = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "print(\"Finished Loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vecPolar[\"test\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elizabeth', 0.6454077959060669),\n",
       " ('isabeau', 0.6140128374099731),\n",
       " ('margrethe', 0.6124271154403687),\n",
       " ('king', 0.6120667457580566),\n",
       " ('regnant', 0.6019624471664429),\n",
       " ('ranavalona', 0.5951443910598755),\n",
       " ('princess', 0.5911498069763184),\n",
       " ('salote', 0.5861144065856934),\n",
       " ('boleyn', 0.5729297399520874),\n",
       " ('consort', 0.5716495513916016)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar(positive=[\"queen\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('shah', 0.5474339723587036),\n",
       " ('king', 0.5038198828697205),\n",
       " ('bhumibol', 0.4365009069442749),\n",
       " ('monarch', 0.42754533886909485),\n",
       " ('isabella', 0.4093480408191681),\n",
       " ('ruler', 0.3890846371650696),\n",
       " ('regent', 0.38861343264579773),\n",
       " ('castile', 0.3847701847553253),\n",
       " ('anula', 0.3807584047317505),\n",
       " ('alfonso', 0.3760611414909363)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vecPolar.wv.most_similar(positive=[\"queen\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166\n",
      "('common', 'royal')\n"
     ]
    }
   ],
   "source": [
    "#print(list_antonym)\n",
    "count=0\n",
    "for i in list_antonym:\n",
    "    #print(type(i))\n",
    "    if i == ('common', 'royal'):\n",
    "        print(count)\n",
    "    count+=1\n",
    "print(list_antonym[1166])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1259,  182,  940, 1308, 1323,  399, 1407,  321, 1363, 1382,  870,\n",
       "        875,  148, 1202,  522, 1400, 1205,  723,  848, 1076,  293, 1374,\n",
       "       1332,  782,  316, 1078, 1074,  429,  981,  653,  397, 1285, 1339,\n",
       "       1236, 1260, 1321, 1275,  975,  995,  862,  411,  612,  442, 1247,\n",
       "       1248,  683,  668,   87,  475,  784])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthogonal_antonymy_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dream', 'real')\n",
      "('insured', 'uninsured')\n",
      "('bounce', 'roll')\n",
      "('beach', 'sea')\n",
      "('both', 'one')\n",
      "('dry', 'river')\n",
      "('fasten', 'loose')\n",
      "('documentary', 'parody')\n",
      "('needle', 'thread')\n",
      "('tell', 'watch')\n",
      "('different', 'same')\n",
      "('center', 'side')\n",
      "('supervised', 'unsupervised')\n",
      "('outside', 'room')\n",
      "('legal', 'unlawful')\n",
      "('royal', 'subject')\n",
      "('morning', 'night')\n",
      "('seen', 'unnoticed')\n",
      "('age', 'young')\n",
      "('sky', 'water')\n",
      "('exceptional', 'modal')\n",
      "('bike', 'car')\n",
      "('everybody', 'nobody')\n",
      "('make', 'nature')\n",
      "('slowness', 'velocity')\n",
      "('meat', 'plant')\n",
      "('professor', 'student')\n",
      "('demolish', 'polish')\n",
      "('action', 'rest')\n",
      "('avoidance', 'intercourse')\n",
      "('queen', 'shah')\n",
      "('root', 'trunk')\n",
      "('handle', 'spout')\n",
      "('huge', 'small')\n",
      "('money', 'payment')\n",
      "('cat', 'mouse')\n",
      "('crater', 'hill')\n",
      "('credit', 'tax')\n",
      "('long', 'second')\n",
      "('back', 'main')\n",
      "('harmony', 'rivalry')\n",
      "('aggressor', 'caretaker')\n",
      "('concentration', 'distribution')\n",
      "('school', 'work')\n",
      "('screen', 'stage')\n",
      "('oceanic', 'shallow')\n",
      "('depart', 'rendezvous')\n",
      "('copycat', 'innovator')\n",
      "('bill', 'coin')\n",
      "('arrogant', 'humble')\n"
     ]
    }
   ],
   "source": [
    "for i in orthogonal_antonymy_vector:\n",
    "    print(list_antonym[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "-8.3285265\n",
      "('iodine', 'water')\n",
      "1069\n",
      "-5.785868\n",
      "('grass', 'water')\n",
      "1219\n",
      "-3.698965\n",
      "('oil', 'water')\n",
      "418\n",
      "-3.6297088\n",
      "('germ', 'iodine')\n",
      "1103\n",
      "-3.483371\n",
      "('boss', 'worker')\n"
     ]
    }
   ],
   "source": [
    "#print(word2vecPolar[\"screenshot\"])\n",
    "word=word2vecPolar[\"water\"]\n",
    "#print(word)\n",
    "#print(list_antonym)\n",
    "#print(orthogonal_antonymy_vector)\n",
    "thisdict = {}\n",
    "\n",
    "indexedList=[]\n",
    "for count, value in enumerate(word):\n",
    "    thisdict[count]= value\n",
    "#print(thisdict)\n",
    "\n",
    "sortedDic=sorted(thisdict.items(), key=lambda item: abs(item[1]))\n",
    "sortedDic.reverse()\n",
    "#print(sortedDic)\n",
    "for i in range(0,5):\n",
    "    cur_Index=sortedDic[i][0]\n",
    "    originalAntonymIndex=orthogonal_antonymy_vector[cur_Index]\n",
    "    print(originalAntonymIndex)\n",
    "    print(sortedDic[i][1])\n",
    "    print(list_antonym[originalAntonymIndex])\n",
    "    #print(sortedDic[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n",
      "finished sentiment task\n"
     ]
    }
   ],
   "source": [
    "######### Task specific code....\n",
    "command_list = ['python3', \n",
    "                    r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Downstream Task\\TREC\\classify_task.py',\n",
    "                     std_nrml_embedding_path,\n",
    "                     '2',\n",
    "                     r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Downstream Task\\TREC\\data\\qa_train_X.pickle',\n",
    "                     r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Downstream Task\\TREC\\data\\qa_train_y.pickle',\n",
    "                     r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Downstream Task\\TREC\\data\\qa_val_X.pickle',\n",
    "                     r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Downstream Task\\TREC\\data\\qa_val_y.pickle',\n",
    "                     r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Downstream Task\\TREC\\data\\qa_test_X.pickle',\n",
    "                     r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Downstream Task\\TREC\\data\\qa_test_y.pickle'\n",
    "                    ]\n",
    "\n",
    "\n",
    "\n",
    "result = subprocess.run(command_list, stdout=subprocess.PIPE, shell=True)\n",
    "print(result.stdout)\n",
    "print('finished sentiment task')\n",
    "################\n",
    "del temp_model\n",
    "    \n",
    "os.remove(std_nrml_embedding_path)\n",
    "os.remove(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
