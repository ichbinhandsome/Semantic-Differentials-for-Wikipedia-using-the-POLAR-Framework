{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forschungspraktikum SS20 \n",
    "# Semantic Differentials for Wikipedia using the POLAR Framework\n",
    "\n",
    "\n",
    "The POLAR Framework is a method that enables Interpretability for pre-trained word embeddings. The goal of this project is to produce word embeddings from a Wikipedia dataset, and deploy the POLAR framework to different categories of words (e.g. Countries, Politicians, Music, etc) in order to explain the semantic associations behind concepts on Wikipedia. An evaluation setup should assess the quality of the semantic differentials produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from gensim.corpora.wikicorpus import WikiCorpus #https://radimrehurek.com/gensim/corpora/wikicorpus.html\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "# for 2d visalization (def display_closestwords_tsnescatterplot):\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#From Polar\n",
    "import gensim\n",
    "from numpy import linalg\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# progress bar for .ipyth \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "from random import shuffle\n",
    "import sys\n",
    "import nltk \n",
    "from nltk.corpus import wordnet \n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "#%load_ext autotime\n",
    "\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for 2d display of vector distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_closestwords_tsnescatterplot(model, word, size):\n",
    "    \n",
    "    arr = np.empty((0,size), dtype='f')\n",
    "    word_labels = [word]\n",
    "    close_words = model.wv.similar_by_word(word)\n",
    "    #print(arr)\n",
    "    #print(np.array([model[word]]))\n",
    "    arr = np.append(arr, np.array([model.wv[word]]), axis=0)\n",
    "    \n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "        plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "        plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "#display_closestwords_tsnescatterplot(word2vec, 'trump', 300) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Antonyms into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAntonyms(model, loadCountryAntonyms,loadStandardAntonyms): \n",
    "    current_model=model #= word2vec\n",
    "    list_antonym = []\n",
    "\n",
    "    loadStandardAntonyms=False\n",
    "    loadCountryAntonyms=True\n",
    "\n",
    "\n",
    "    if loadCountryAntonyms== True:\n",
    "        \n",
    "        with open(r'Antonym_sets/country50.txt') as fp:\n",
    "            for line in fp:\n",
    "                parts = line.split()\n",
    "                if parts[1]!=' ':\n",
    "                    word1 = parts[0]\n",
    "                    word2 = parts[1]\n",
    "                    if word1 in current_model and word2 in current_model:\n",
    "                        list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#POLAR Framework code (https://github.com/Sandipan99/POLAR)\n",
    "    if loadStandardAntonyms==True:\n",
    "        with open(r'Antonym_sets/LenciBenotto.val') as fp:\n",
    "            for line in fp:\n",
    "                parts = line.split()\n",
    "                if parts[3]=='antonym':\n",
    "                    word1 = parts[0].split('-')[0]\n",
    "                    word2 = parts[1].split('-')[0]\n",
    "                    if word1 in current_model and word2 in current_model:\n",
    "                        list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "\n",
    "\n",
    "        with open(r'Antonym_sets/LenciBenotto.test') as fp:\n",
    "            for line in fp:\n",
    "                parts = line.split()\n",
    "                if parts[3]=='antonym':\n",
    "                    word1 = parts[0].split('-')[0]\n",
    "                    word2 = parts[1].split('-')[0]\n",
    "                    if word1 in current_model and word2 in current_model:\n",
    "                        list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "\n",
    "        with open(r'Antonym_sets/EVALution.val') as fp:\n",
    "            for line in fp:\n",
    "                parts = line.split()\n",
    "                if parts[3]=='antonym':\n",
    "                    word1 = parts[0].split('-')[0]\n",
    "                    word2 = parts[1].split('-')[0]\n",
    "                    if word1 in current_model and word2 in current_model:\n",
    "                        list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "\n",
    "        with open(r'Antonym_sets/EVALution.test') as fp:\n",
    "            for line in fp:\n",
    "                parts = line.split()\n",
    "                if parts[3]=='antonym':\n",
    "                    word1 = parts[0].split('-')[0]\n",
    "                    word2 = parts[1].split('-')[0]\n",
    "                    if word1 in current_model and word2 in current_model:\n",
    "                        list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "\n",
    "\n",
    "    list_antonym = list(dict.fromkeys(list_antonym).keys())\n",
    "    return list_antonym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing similarty between Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POLAR Framework code (https://github.com/Sandipan99/POLAR) :\n",
    "def computeFinalAntonymsSimilariy(list_antonym): \n",
    "    similarity_matrix = defaultdict(list)\n",
    "    #pre processing of antonyms\n",
    "    for each_pair in tqdm(list_antonym):\n",
    "        word1 = each_pair[0]\n",
    "        word2 = each_pair[1]\n",
    "        if word1 < word2: ## welches wort länger ist\n",
    "            similarity_matrix[word1].append(word2)                  #defaultdict(<class 'list'>, {'exclude': ['inscribe']})\n",
    "            #print(similarity_matrix)\n",
    "        else:\n",
    "            similarity_matrix[word2].append(word1)\n",
    "\n",
    "    # computing similarity of antonyms:\n",
    "    all_similarity = defaultdict(dict)\n",
    "    for each_key in tqdm(similarity_matrix):\n",
    "        for each_value in similarity_matrix[each_key]:\n",
    "    #       cosine_similarity\n",
    "            all_similarity[each_key][each_value] = abs(cosine_similarity([current_model.wv[each_key]],[current_model.wv[each_value]])[0][0])\n",
    "\n",
    "    \n",
    "    final_antonym_list = []\n",
    "    for index_counter, each_key in enumerate(tqdm(all_similarity)):\n",
    "        listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "        final_antonym_list.append((each_key, listofTuples[0][0]))\n",
    "\n",
    "    list_antonym = final_antonym_list\n",
    "    return list_antonym, all_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the ORTHOGONAL DIMENSION Order\n",
    "# Subspace selection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from scipy.spatial.distance import cosine as scipy_cosine\n",
    "random.seed(42)\n",
    "\n",
    "def computedimension_similarity_matrix(antonymy_vector):\n",
    "    t1 = np.array(antonymy_vector)\n",
    "    dimension_similarity_matrix = defaultdict(dict)\n",
    "    for index_1, each_dim1 in enumerate(tqdm(t1)):\n",
    "        for index_2, each_dim2 in enumerate(t1):\n",
    "            dimension_similarity_matrix[index_1][index_2] = abs(1-scipy_cosine(each_dim1, each_dim2))\n",
    "    return dimension_similarity_matrix\n",
    "        \n",
    "def get_set_score(final_list, each_dim,antonymy_vector):\n",
    "    final_output = 0.0\n",
    "    dimension_similarity_matrix=computedimension_similarity_matrix(antonymy_vector)\n",
    "    for each_vec in final_list:\n",
    "        final_output += dimension_similarity_matrix[each_vec][each_dim]\n",
    "    return final_output/(len(final_list))\n",
    "        \n",
    "def select_subset_dimension(dim_vector, num_dim):\n",
    "    working_list = np.array(dim_vector)\n",
    "    \n",
    "    working_position_index = [i for i in range(working_list.shape[0])]\n",
    "    final_position_index = []\n",
    "    \n",
    "\n",
    "    print('working list is ready, shape', working_list.shape)\n",
    "    sel_dim = random.randrange(0, working_list.shape[0])\n",
    "\n",
    "    final_position_index.append(sel_dim)\n",
    "    \n",
    "    working_position_index.remove(sel_dim)\n",
    "\n",
    "    \n",
    "    for test_count in tqdm(range(num_dim-1)):\n",
    "        min_dim = None\n",
    "        min_score = 1000\n",
    "        for temp_index, each_dim in enumerate(working_position_index):\n",
    "#             print(each_dim)\n",
    "            temp_score = get_set_score(final_position_index, each_dim,dim_vector)\n",
    "            if temp_score< min_score:\n",
    "                min_score= temp_score\n",
    "                min_dim = each_dim\n",
    "        #print(test_count,min_dim)\n",
    "        final_position_index.append(min_dim)\n",
    "        working_position_index.remove(min_dim)\n",
    "#         print(working_list.shape, len(final_list))\n",
    "    return final_position_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform into Polar space function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_antonym_space(current_model, output_file_path, binary, current_antonymy_vector_inverse):\n",
    "    embedding_size = orthogonal_antonymy_vector.shape[0] #current_antonymy_vector_inverse.shape[0]   ##CHANGE THIS ACCORDINGLY!!!\n",
    "    print('New model size is',len(current_model.wv.vocab), embedding_size)\n",
    "\n",
    "    temp_file = None\n",
    "    \n",
    "    if binary:\n",
    "        temp_file = open(output_file_path,'wb')\n",
    "        temp_file.write(str.encode(str(len(current_model.wv.vocab))+' '+str(embedding_size)+'\\n'))\n",
    "    else:\n",
    "        temp_file = open(output_file_path,'w')\n",
    "        temp_file.write(str(len(current_model.wv.vocab))+' '+str(embedding_size)+'\\n')\n",
    "\n",
    "    total_words = 0\n",
    "    for each_word in tqdm(current_model.wv.vocab):\n",
    "        total_words += 1\n",
    "        if binary:\n",
    "            temp_file.write(str.encode(each_word+' '))\n",
    "        else:\n",
    "            temp_file.write(each_word+' ')\n",
    "\n",
    "        new_vector = np.matmul(current_antonymy_vector_inverse,current_model[each_word])\n",
    "\n",
    "        new_vector = new_vector/linalg.norm(new_vector)\n",
    "\n",
    "        \n",
    "        \n",
    "        if binary:\n",
    "            temp_file.write(new_vector)\n",
    "            temp_file.write(str.encode('\\n'))\n",
    "        else:\n",
    "            temp_file.write(str(new_vector))\n",
    "            temp_file.write('\\n')\n",
    "\n",
    "\n",
    "    temp_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Normal transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_normal_dist_model(model, new_filename):\n",
    "    embedding_matrix = []\n",
    "    embedding_vocab = []\n",
    "\n",
    "    temp_file = open(new_filename,'wb')\n",
    "    temp_file.write(str.encode(str(model.vectors.shape[0])+' '+str(model.vectors.shape[1])+'\\n'))\n",
    "    \n",
    "    for each_word in tqdm(model.wv.vocab):\n",
    "        embedding_matrix.append(model[each_word])\n",
    "        embedding_vocab.append(each_word)\n",
    "    \n",
    "    embedding_matrix = np.array(embedding_matrix)\n",
    "    \n",
    "    print('The shape of embedding matrix is {}'.format(embedding_matrix.shape))\n",
    "    \n",
    "    norm_embedding_matrix = (embedding_matrix - embedding_matrix.mean(0))/ embedding_matrix.std(0)\n",
    "    \n",
    "    for word_counter, each_word in enumerate(tqdm(embedding_vocab)):\n",
    "#         assert each_word==embedding_vocab[word_counter],'Not matching!!!'\n",
    "        \n",
    "        temp_file.write(str.encode(each_word+' '))\n",
    "        new_vector = norm_embedding_matrix[word_counter]\n",
    "        temp_file.write(new_vector)\n",
    "        temp_file.write(str.encode('\\n'))\n",
    "        \n",
    "    del embedding_matrix\n",
    "    del embedding_vocab\n",
    "    temp_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real code starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only run next cell if a new Wikipedia dataset is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if its your first time running this NodeBook\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6ceb52c4c54e0fa5987e26744732d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=True, description='trainNewModel'), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "def myFunction(trainNewModel):\n",
    "    return trainNewModel\n",
    "print(\"Check if its your first time running this NodeBook\")\n",
    "trainNewModel=interact(myFunction, trainNewModel=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainNewModel=True\n",
    "if trainNewModel==True:\n",
    "    wiki = WikiCorpus(r\"DataSets/enwiki-20200401-pages-articles1.xml-p1p30303.bz2\", \n",
    "                  lemmatize=False, dictionary={})\n",
    "    #stores the preprocessed dataset, which reduces runtime for the future\n",
    "    wiki.save('SavedWord2Vec/wiki.corpus')\n",
    "    \n",
    "    sentences = list(wiki.get_texts()) #Iterate over the dump, yielding a list of tokens for each article that passed the length and namespace filtering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "size: The number of dimensions of the embeddings and the default is 100.\n",
    "\n",
    "window: The maximum distance between a target word and words around the target word. The default window is 5.\n",
    "\n",
    "min_count: The minimum count of words to consider when training the model; words with occurrence less than this \n",
    "count will be ignored. The default for min_count is 5.\n",
    "\n",
    "workers: The number of partitions during training and the default workers is 3.\n",
    "\n",
    "sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model from file...\n",
      "Finished Loading\n"
     ]
    }
   ],
   "source": [
    "### Compute the Word2Vec model usint following parameters#\n",
    "# Currently using CBOW for testing because of performance issues of skip gram on my laptop #\n",
    "if trainNewModel==True:\n",
    "    print(\"Training Word2Vec model ...\")\n",
    "    params = {'size': 300, 'window': 5, 'min_count': 5, \n",
    "          'workers': max(1, multiprocessing.cpu_count() - 1), 'sample': 1E-3,'sg' : 1}\n",
    "    word2vec = Word2Vec(sentences, **params)\n",
    "    ### Save Word2Vec model to file ###\n",
    "    word2vec.save('SavedWord2Vec/wiki.word2vec.model')\n",
    "    print(\"Finished Training\")\n",
    "else:\n",
    "    ### Loading finished Word2Vec model to memory from file ###\n",
    "    print(\"Loading Word2Vec model from file...\")\n",
    "    word2vec = Word2Vec.load('SavedWord2Vec/wiki.word2vec.model')\n",
    "    print(\"Finished Loading\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## King- Man + Woman = Queen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarityNameVector(vec, name, model):\n",
    "    \n",
    "    cosineSimilarity = np.dot(vec, model.wv[name])/(np.linalg.norm(vec)* np.linalg.norm(model.wv[name]))\n",
    "    return cosineSimilarity\n",
    "\n",
    "#print(similarityNameVector(ourQueen,'queen'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smiliarity of king and queen: 0.6234725\n",
      "Smiliarity of transformed king and queen: 0.57965255\n",
      "The most similar words to out transformation of King to Queen are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('king', 0.7423334717750549),\n",
       " ('queen', 0.5796525478363037),\n",
       " ('woman', 0.517516016960144),\n",
       " ('regnant', 0.5111746788024902),\n",
       " ('isabeau', 0.5028153657913208),\n",
       " ('throne', 0.5016829967498779),\n",
       " ('consort', 0.4985124170780182),\n",
       " ('chlothar', 0.48926517367362976),\n",
       " ('electress', 0.48813995718955994),\n",
       " ('theodelinda', 0.4879298210144043)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Smiliarity of king and queen:\", word2vec.wv.similarity('king', 'queen'))\n",
    "ourQueen= word2vec.wv['king'] - word2vec.wv['man'] + word2vec.wv['woman']\n",
    "print(\"Smiliarity of transformed king and queen:\", similarityNameVector(ourQueen,'queen',word2vec))\n",
    "\n",
    "print(\"The most similar words to out transformation of King to Queen are:\")\n",
    "word2vec.wv.most_similar(positive=[ourQueen], topn=10) # cosine similarity\n",
    "#word2vec.wv.most_similar(positive=[\"king\"], topn=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar words to out transformation of France to Japan are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tokyo', 0.7715462446212769),\n",
       " ('japan', 0.7495639324188232),\n",
       " ('osaka', 0.6108940839767456),\n",
       " ('nagoya', 0.5861287117004395),\n",
       " ('kyushu', 0.5758974552154541),\n",
       " ('hyōgo', 0.5652197003364563),\n",
       " ('miyagi', 0.5611051321029663),\n",
       " ('ehime', 0.5560809373855591),\n",
       " ('saitama', 0.5548993349075317),\n",
       " ('ryukyu', 0.5532717704772949)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ourJapan= word2vec.wv['france'] - word2vec.wv['paris'] + word2vec.wv['tokyo']\n",
    "print(\"The most similar words to out transformation of France to Japan are:\")\n",
    "word2vec.wv.most_similar(positive=[ourJapan], topn=10) # cosine similarity\n",
    "#word2vec.wv.most_similar(positive=[\"king\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull functions:\n",
    "\n",
    "word2vec.wv.similarity('star', 'hollywood')  \n",
    "word2vec.wv.most_similar('afghanistan')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online training / Resuming training\n",
    "\n",
    "Advanced users can load a model and continue training it with more sentences and new vocabulary words:\n",
    "\n",
    "model = gensim.models.Word2Vec.load(temporary_filepath)\n",
    "more_sentences = [\n",
    "    ['Advanced', 'users', 'can', 'load', 'a', 'model',\n",
    "     'and', 'continue', 'training', 'it', 'with', 'more', 'sentences']\n",
    "]\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "\n",
    "cleaning up temporary file\n",
    "import os\n",
    "os.remove(temporary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POLAR Framework code (https://github.com/Sandipan99/POLAR) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation to polar space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the task score for different dimension size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding_path(current_model, embedding_path, binary, antonym_vector, curr_dim):\n",
    "    curr_antonym_vector = antonymy_vector[antonym_vector[:curr_dim]]\n",
    "    curr_antonymy_vector_inverse = np.linalg.pinv(np.transpose(curr_antonym_vector))\n",
    "    #transform to polar space and write3s to file\n",
    "    transform_to_antonym_space(current_model, embedding_path, binary,curr_antonymy_vector_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put it all together\n",
    "\n",
    "current_model=word2vec\n",
    "\n",
    "list_antonym=loadAntonyms(model=word2vec, loadCountryAntonyms=True,loadStandardAntonyms=False)\n",
    "\n",
    "list_antonym, all_similarity=computeFinalAntonymsSimilariy(list_antonym)\n",
    "\n",
    "num_antonym = 33#1468\n",
    "## Find the antonym difference vectors\n",
    "antonymy_vector = []\n",
    "for each_word_pair in list_antonym:\n",
    "    antonymy_vector.append(current_model.wv[each_word_pair[0]]- current_model.wv[each_word_pair[1]])\n",
    "antonymy_vector = np.array(antonymy_vector)\n",
    "print(antonymy_vector.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Get the orthogonal dimension order\n",
    "orthogonal_antonymy_vector =np.array(select_subset_dimension(antonymy_vector, num_antonym))  \n",
    "\n",
    "\n",
    "antonym_vector_method = orthogonal_antonymy_vector # we used only the orthogonality method from the original paper\n",
    "\n",
    "curr_dim = len(antonym_vector_method)#dim_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model size is 147612 33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b367b2ed04f2428286cd9075dda57ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=147612), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading the model\n",
      "loading done..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d0876cae3e41b69c4ddd8ba8584a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=147612), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The shape of embedding matrix is (147612, 33)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ceafc3f158c4c5d8761c148ac8e8c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=147612), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "embedding_path = r'SavedWord2Vec/polarEmbedding'+str(curr_dim)+'.bin'\n",
    "# model is transformed to polar space and stored as a file\n",
    "generate_embedding_path(current_model, embedding_path,True,antonym_vector_method, curr_dim)\n",
    "\n",
    "# Model is loaded from file to memory\n",
    "print('loading the model')\n",
    "temp_model = gensim.models.KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n",
    "print('loading done..')\n",
    "\n",
    "#Model is tranformed to standard normal and stored to file\n",
    "std_nrml_embedding_path = r'SavedWord2Vec/polarEmbedding'+str(curr_dim)+'_StdNrml.bin'\n",
    "standard_normal_dist_model(temp_model, std_nrml_embedding_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Loading\n"
     ]
    }
   ],
   "source": [
    "#curr_dim=500\n",
    "path=r'polarEmbedding'+str(curr_dim)+'_StdNrml.bin'\n",
    "#f = open(path, \"r+b\")\n",
    "#f.seek(0)\n",
    "#print(f)\n",
    "word2vecPolar = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "print(\"Finished Loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vecPolar[\"test\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elizabeth', 0.6422714591026306),\n",
       " ('king', 0.6234725713729858),\n",
       " ('isabeau', 0.6121804714202881),\n",
       " ('ranavalona', 0.6099308133125305),\n",
       " ('margrethe', 0.6072167158126831),\n",
       " ('salote', 0.5936083793640137),\n",
       " ('consort', 0.5914666652679443),\n",
       " ('boleyn', 0.5876452922821045),\n",
       " ('regnant', 0.5866461992263794),\n",
       " ('isabella', 0.5862259864807129)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar(positive=[\"queen\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ewen', 0.7431647181510925),\n",
       " ('bengough', 0.7329139709472656),\n",
       " ('stoiber', 0.7286815643310547),\n",
       " ('seaga', 0.7137287855148315),\n",
       " ('haig', 0.7072637677192688),\n",
       " ('adlai', 0.7067761421203613),\n",
       " ('lenihan', 0.7049410343170166),\n",
       " ('mckinnon', 0.7025044560432434),\n",
       " ('appleby', 0.7014561295509338),\n",
       " ('hartz', 0.6994128227233887)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vecPolar.wv.most_similar(positive=[\"angela\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar words to out transformation of France to Japan are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('unborn', 0.7739344835281372),\n",
       " ('typhoid', 0.7263432741165161),\n",
       " ('triable', 0.7203752398490906),\n",
       " ('kuraka', 0.7203114032745361),\n",
       " ('ransomed', 0.7131144404411316),\n",
       " ('disinherit', 0.7128853797912598),\n",
       " ('themself', 0.7108350992202759),\n",
       " ('stillborn', 0.7067704796791077),\n",
       " ('briseis', 0.7037810683250427),\n",
       " ('divulge', 0.6928963661193848)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ourTokyo= word2vecPolar['king'] - word2vecPolar['man'] + word2vecPolar['woman']\n",
    "print(\"The most similar words to out transformation of France to Japan are:\")\n",
    "word2vecPolar.most_similar(positive=[ourTokyo], topn=10) # cosine similarity\n",
    "#word2vec.wv.most_similar(positive=[\"king\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-6d0a5c14732e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcount\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_antonym\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1166\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#print(list_antonym)\n",
    "count=0\n",
    "for i in list_antonym:\n",
    "    #print(type(i))\n",
    "    if i == ('common', 'royal'):\n",
    "        print(count)\n",
    "    count+=1\n",
    "print(list_antonym[1166])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orthogonal_antonymy_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('black', 'white')\n",
      "('poor', 'rich')\n",
      "('asia', 'europa')\n",
      "('busy', 'calm')\n",
      "('desert', 'vegetation')\n",
      "('first', 'third')\n",
      "('north', 'south')\n",
      "('hindi', 'muslim')\n",
      "('green', 'grey')\n",
      "('modern', 'traditional')\n",
      "('city', 'land')\n",
      "('china', 'russia')\n",
      "('long', 'wide')\n",
      "('cosmopolitan', 'limited')\n",
      "('baltic', 'scandinavian')\n",
      "('large', 'small')\n",
      "('civilized', 'uncivilized')\n",
      "('democratic', 'monarchic')\n",
      "('cold', 'hot')\n",
      "('developed', 'undeveloped')\n",
      "('america', 'china')\n",
      "('hispanic', 'latino')\n",
      "('catholic', 'muslim')\n",
      "('big', 'tiny')\n",
      "('english', 'russian')\n",
      "('east', 'west')\n",
      "('white', 'yellow')\n",
      "('development', 'farming')\n",
      "('blue', 'grey')\n",
      "('portuguese', 'spanish')\n",
      "('buddhism', 'catholic')\n",
      "('brown', 'white')\n",
      "('chinese', 'english')\n"
     ]
    }
   ],
   "source": [
    "for i in orthogonal_antonymy_vector:\n",
    "    print(list_antonym[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5513357   0.2992909  -0.9792398  -1.1928256  -1.0671536   0.64150095\n",
      "  1.1393831   0.76281327  1.2610471   0.32421577 -2.5613334  -2.2042937\n",
      " -0.05838816 -0.02575333  2.0912066   0.65878993 -0.7909302  -1.0279247\n",
      " -0.21182877 -1.3658224  -0.6284247   0.6461692  -0.19993071 -0.69146174\n",
      "  1.9355266   0.23070632 -0.120572   -0.6441405   0.40501508 -0.82125735\n",
      "  0.22921325  0.22414668  2.231083  ]\n",
      "-2.5613334\n",
      "('city', 'land')\n",
      "2.231083\n",
      "('chinese', 'english')\n",
      "-2.2042937\n",
      "('china', 'russia')\n",
      "2.0912066\n",
      "('baltic', 'scandinavian')\n",
      "1.9355266\n",
      "('english', 'russian')\n"
     ]
    }
   ],
   "source": [
    "#print(word2vecPolar[\"screenshot\"])\n",
    "word=word2vecPolar[\"water\"]\n",
    "print(word)\n",
    "#print(list_antonym)\n",
    "#print(orthogonal_antonymy_vector)\n",
    "thisdict = {}\n",
    "\n",
    "#indexedList=[]\n",
    "for count, value in enumerate(word):\n",
    "    thisdict[count]= value\n",
    "#print(thisdict)\n",
    "\n",
    "sortedDic=sorted(thisdict.items(), key=lambda item: abs(item[1]))\n",
    "sortedDic.reverse()\n",
    "#print(sortedDic)\n",
    "for i in range(0,5):\n",
    "    cur_Index=sortedDic[i][0]\n",
    "    originalAntonymIndex=orthogonal_antonymy_vector[cur_Index]\n",
    "    #print(originalAntonymIndex)\n",
    "    print(sortedDic[i][1])\n",
    "    print(list_antonym[originalAntonymIndex])\n",
    "    #print(sortedDic[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pp\n",
    "def getMeaningOfWord(strWord,antonymy_vector, model, numberPolar ):\n",
    "    #print(antonymy_vector)\n",
    "    word=model[strWord]\n",
    "    thisdict = {}\n",
    "    for count, value in enumerate(word):\n",
    "        thisdict[count]= value\n",
    "    sortedDic=sorted(thisdict.items(), key=lambda item: abs(item[1]))\n",
    "    sortedDic.reverse()\n",
    "   \n",
    "    for i in range(0,numberPolar):\n",
    "        cur_Index=sortedDic[i][0]\n",
    "        originalAntonymIndex=antonymy_vector[cur_Index]\n",
    "        #print(cur_Index)\n",
    "        #print(originalAntonymIndex)\n",
    "        #print(originalAntonymIndex)\n",
    "        print(sortedDic[i][1])\n",
    "        cur_value =sortedDic[i][1]\n",
    "        leftPolar=list_antonym[originalAntonymIndex][0]\n",
    "        rightPolar=list_antonym[originalAntonymIndex][1]\n",
    "        plotPolar(rightPolar, leftPolar, cur_value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPolar(left, right, value):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_figheight(1)\n",
    "    \n",
    "\n",
    "    if abs(value)>10:\n",
    "        fig.set_figwidth(20)\n",
    "        ar = np.arange(-20,21)   \n",
    "    else:\n",
    "        fig.set_figwidth(10)\n",
    "        ar = np.arange(-10,11)\n",
    "    ax1.plot(ar, np.zeros_like(ar) + 0, '.')\n",
    "    ax1.plot(value, 0, 'd', linewidth=2, markersize=20, color='r')\n",
    "    ax1.set_ylabel(left, color='b',rotation=0, size=20, labelpad=50)\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xticks(ar)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['left'].set_visible(False)\n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel(right, color='r',rotation=0, size=20, labelpad=50)\n",
    "    ax2.set_yticks([])\n",
    "    ax1.grid(False)\n",
    "    ax2.grid(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['left'].set_visible(False)\n",
    "    ax2.spines['bottom'].set_visible(False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2vecPolar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c0a0bfd1cbda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetMeaningOfWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"china\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morthogonal_antonymy_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vecPolar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'word2vecPolar' is not defined"
     ]
    }
   ],
   "source": [
    "getMeaningOfWord(\"china\",orthogonal_antonymy_vector, word2vecPolar, 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n",
      "finished sentiment task\n"
     ]
    }
   ],
   "source": [
    "######### Task specific code....\n",
    "command_list = ['python3', \n",
    "                    r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Downstream Task\\TREC\\classify_task.py',\n",
    "                     std_nrml_embedding_path,\n",
    "                     '2',\n",
    "                     r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Downstream Task\\TREC\\data\\qa_train_X.pickle',\n",
    "                     r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Downstream Task\\TREC\\data\\qa_train_y.pickle',\n",
    "                     r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Downstream Task\\TREC\\data\\qa_val_X.pickle',\n",
    "                     r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Downstream Task\\TREC\\data\\qa_val_y.pickle',\n",
    "                     r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Downstream Task\\TREC\\data\\qa_test_X.pickle',\n",
    "                     r'C:\\Users\\Jan\\Desktop\\PraktikumSS20\\POLAR-master\\Downstream Task\\TREC\\data\\qa_test_y.pickle'\n",
    "                    ]\n",
    "\n",
    "\n",
    "\n",
    "result = subprocess.run(command_list, stdout=subprocess.PIPE, shell=True)\n",
    "print(result.stdout)\n",
    "print('finished sentiment task')\n",
    "################\n",
    "del temp_model\n",
    "    \n",
    "os.remove(std_nrml_embedding_path)\n",
    "os.remove(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
